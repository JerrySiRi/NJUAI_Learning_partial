{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZthb8N2n576"
      },
      "source": [
        "# 1. Retrieval Augmented In-context Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAEb0cV9GJFD"
      },
      "source": [
        "- Please choose \"Runtime Type\" = GPU in Colab for running this notebook (Runtime > Change Runtime Type > T4 GPU).\n",
        "- You are free to choose to use Google Colab or Kaggle to complete this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3KJrkx9x0ft"
      },
      "source": [
        "## 1.1 Contextual Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foCqK3uOJdbz"
      },
      "outputs": [],
      "source": [
        "# Step 0. Prepare the environment\n",
        "!pip install InstructorEmbedding sentence-transformers datasets scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIdUKCaRo9x7"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data/classification\n",
        "!wget -O data/classification/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/train.txt\n",
        "!wget -O data/classification/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/dev.txt\n",
        "!wget -O data/classification/test-blind.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/test-blind.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OEUTe4gnmLM"
      },
      "outputs": [],
      "source": [
        "# Step 1. Declare the model & Example usage\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"hkunlp/instructor-base\")\n",
        "embeddings = model.encode(\n",
        "    [\n",
        "        \"Dynamical Scalar Degree of Freedom in Horava-Lifshitz Gravity\",\n",
        "        \"Comparison of Atmospheric Neutrino Flux Calculations at Low Energies\",\n",
        "        \"Fermion Bags in the Massive Gross-Neveu Model\",\n",
        "        \"QCD corrections to Associated t-tbar-H production at the Tevatron\",\n",
        "    ],\n",
        "    prompt=\"Represent the Medicine sentence for clustering: \",\n",
        "    show_progress_bar=True,\n",
        ")\n",
        "\n",
        "print(embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym3s3JmGjF3T"
      },
      "outputs": [],
      "source": [
        "# Step 2. Load data & Extract embeddings\n",
        "# It may takes about two hours for CPU, or you can switch to GPU runtime for faster inference.\n",
        "def load_data(file):\n",
        "    r\"\"\"Load your custom data for training or evaluation.\n",
        "\n",
        "    Args:\n",
        "        file (str): the path of your data\n",
        "\n",
        "    Returns:\n",
        "        tuple: a tuple containing two elements:\n",
        "           - embedding: The embedding of your data, should in shape (N, 768)\n",
        "          - label: List of labels, should in shape (N,)\n",
        "    \"\"\"\n",
        "    with open(file) as f:\n",
        "        data = [line.strip().split('\\t') for line in f]\n",
        "    sentences = [line[1] for line in data]\n",
        "    X = model.encode(sentences, prompt=\"Represent the sentence for sentiment analysis: \", show_progress_bar=True)\n",
        "    y = [int(line[0]) for line in data]\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = load_data('data/classification/train.txt')\n",
        "X_val, y_val = load_data('data/classification/dev.txt')\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {len(y_train)}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {len(y_val)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tT2EAxuwdpb"
      },
      "source": [
        "And train a logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMJlH3gfr7AF"
      },
      "outputs": [],
      "source": [
        "# Step 3: Train a logistic regression model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGJytlto8rri"
      },
      "outputs": [],
      "source": [
        "# Step 4: Evaluate the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myMyyvzbeOeS"
      },
      "source": [
        "## 1.2 Retrieve Relevant Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOtq7JAoeS3s"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def load_train_data():\n",
        "    \"\"\"Loads the GSM8k train dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries containing questions, cot answers, and short answers.\n",
        "    \"\"\"\n",
        "    ds = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
        "    examples = [{\"question\": example[\"question\"], \"answer\": example[\"answer\"]} for example in ds]\n",
        "    for example in examples:\n",
        "        example[\"short_answer\"] = re.sub(r\"(\\d),(\\d)\", r\"\\1\\2\", example[\"answer\"].split(\"####\")[1].strip())\n",
        "        example[\"cot_answer\"] = re.sub(r\"\\<\\<.*?\\>\\>\", \"\", example[\"answer\"].split(\"####\")[0].strip()) \\\n",
        "            + \" So the answer is \" + example[\"short_answer\"] + \".\"\n",
        "    return examples\n",
        "\n",
        "def load_test_data():\n",
        "    \"\"\"Loads the first 30 examples of the GSM8k test dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries containing questions and answers.\n",
        "    \"\"\"\n",
        "    ds = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
        "    examples = [{\"question\": example[\"question\"], \"answer\": example[\"answer\"].split(\"####\")[1].strip()} for example in ds]\n",
        "    for example in examples:\n",
        "        example[\"answer\"] = re.sub(r\"(\\d),(\\d)\", r\"\\1\\2\", example[\"answer\"])\n",
        "    return examples[:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFV3OK5lxNCU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def retrieve_examples():\n",
        "    \"\"\"Retrieve top-20 in-context examples from GSM8K training set for each testing examples.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping testing questions to a list of top-20 training examples.\n",
        "    \"\"\"\n",
        "    raise NotImplmentedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pvcVHCgH_nD"
      },
      "outputs": [],
      "source": [
        "RETRIVED_EXAMPLES = retrieve_examples()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80uOvAFZsG_d"
      },
      "source": [
        "Note: The following retrieval augmented generation does not require a GPU. Please consider saving and downloading the examples you retrieve from the left file tab so that you will not be hindered by Colab GPU restrictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h8_pRvcxDm8"
      },
      "outputs": [],
      "source": [
        "with open(\"retrieved_examples.json\", \"w\") as fout:\n",
        "    json.dump(RETRIVED_EXAMPLES, fout)\n",
        "\n",
        "with open(\"retrieved_examples.json\", \"r\") as fin:\n",
        "    RETRIVED_EXAMPLES = json.load(fin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj8qq9zreTNG"
      },
      "source": [
        "## 1.3 Generation with Huggingface Inference API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nkG7MmCsEVR"
      },
      "source": [
        "We will use LLM by querying huggingface inference api so we do not need GPU for the following code. Please generate HF_TOKEN at [hf.co/settings/tokens](hf.co/settings/tokens) and set as environment varible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmOrunUbr4QS"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Dict, Any\n",
        "import os\n",
        "import json\n",
        "import backoff\n",
        "import evaluate\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"HF_TOKEN\"] = \"<YOUR TOKEN>\"\n",
        "\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "class LLM(object):\n",
        "    def __init__(self, model_name=\"codellama/CodeLlama-7b-hf\"):\n",
        "        self.model_name = model_name\n",
        "        self.api_url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
        "        self.headers = {\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"}\n",
        "\n",
        "    @backoff.on_exception(backoff.expo, requests.exceptions.RequestException, max_time=60)\n",
        "    def generate(self, prompts: List[str], **kwargs) -> List[str]:\n",
        "        outputs = []\n",
        "\n",
        "        def query(payload):\n",
        "            response = requests.post(self.api_url, headers=self.headers, json=payload)\n",
        "            if response.status_code != 200:\n",
        "                raise ValueError(f\"Request failed with code {response.status_code}, {response.text}\")\n",
        "            return response.json()\n",
        "\n",
        "        for prompt in prompts:\n",
        "            data = query(\n",
        "                {\n",
        "                    \"inputs\": prompt,\n",
        "                    \"parameters\": { \"max_new_tokens\": 256, \"stop\": [\"Question:\"]},\n",
        "                }\n",
        "            )\n",
        "\n",
        "            outputs.append(data[0]['generated_text'])\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yYZEFsJuZex"
      },
      "outputs": [],
      "source": [
        "llm = LLM(\"codellama/CodeLlama-7b-hf\")\n",
        "print(llm.generate([\"Explain the importance of low latency LLMs\", \"What is the capital of France?\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y-4jKTXvEcO"
      },
      "source": [
        "Please adapt your GSM8KCoTEvaluator for this API-based LLM. And report the performance of 8-shot chain-of-thought prompting on first 30 examples of GSM8K."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgD2XUgWvZEi"
      },
      "outputs": [],
      "source": [
        "class Evaluator(ABC):\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    @abstractmethod\n",
        "    def load_data(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def build_prompts(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def postprocess_output(self, output: str) -> str:\n",
        "        pass\n",
        "\n",
        "    def generate_completions(self, prompts: List[str], batch_size=2) -> List[str]:\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, batch_size=4, n_shot=8, save_dir=\"outputs\"):\n",
        "        dataset = self.load_data()\n",
        "        prompts = self.build_prompts(dataset, n_shot)\n",
        "        outputs = self.generate_completions(prompts, batch_size=batch_size)\n",
        "\n",
        "        predictions = []\n",
        "        for i, (example, prompt, output) in enumerate(zip(dataset, prompts, outputs)):\n",
        "            prediction = {\n",
        "                \"task_id\": example.get(\"task_id\", f\"task_{i}\"),\n",
        "                \"prompt\": prompt,\n",
        "                \"completion\": output,\n",
        "                \"prediction\": self.postprocess_output(output),\n",
        "            }\n",
        "            predictions.append(prediction)\n",
        "\n",
        "        # Save predictions to file\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        prediction_save_path = os.path.join(save_dir, f\"{type(self).__name__}_predictions.jsonl\")\n",
        "        with open(prediction_save_path, \"w\") as fout:\n",
        "            for pred in predictions:\n",
        "                fout.write(json.dumps(pred) + \"\\n\")\n",
        "\n",
        "        # Calculate metrics and print results\n",
        "        results = self.calculate_metrics(predictions, dataset)\n",
        "        print(f\"Results for {type(self).__name__}: {results}\")\n",
        "\n",
        "    @abstractmethod\n",
        "    def calculate_metrics(self):\n",
        "        pass\n",
        "\n",
        "GSM_EXAMPLARS = [\n",
        "    {\n",
        "        \"question\": \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n",
        "        \"cot_answer\": \"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is 6.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\\"\\\"\\\"\\n    trees_initial = 15\\n    trees_after = 21\\n    trees_added = trees_after - trees_initial\\n    result = trees_added\\n    return result\",\n",
        "        \"short_answer\": \"6\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n",
        "        \"cot_answer\": \"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is 5.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\\"\\\"\\\"\\n    cars_initial = 3\\n    cars_arrived = 2\\n    total_cars = cars_initial + cars_arrived\\n    result = total_cars\\n    return result\",\n",
        "        \"short_answer\": \"5\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n",
        "        \"cot_answer\": \"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is 39.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\\"\\\"\\\"\\n    leah_chocolates = 32\\n    sister_chocolates = 42\\n    total_chocolates = leah_chocolates + sister_chocolates\\n    chocolates_eaten = 35\\n    chocolates_left = total_chocolates - chocolates_eaten\\n    result = chocolates_left\\n    return result\",\n",
        "        \"short_answer\": \"39\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\",\n",
        "        \"cot_answer\": \"Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is 8.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\\"\\\"\\\"\\n    jason_lollipops_initial = 20\\n    jason_lollipops_after = 12\\n    denny_lollipops = jason_lollipops_initial - jason_lollipops_after\\n    result = denny_lollipops\\n    return result\",\n",
        "        \"short_answer\": \"8\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\",\n",
        "        \"cot_answer\": \"Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is 9.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\\"\\\"\\\"\\n    toys_initial = 5\\n    mom_toys = 2\\n    dad_toys = 2\\n    total_received = mom_toys + dad_toys\\n    total_toys = toys_initial + total_received\\n    result = total_toys\\n    return result\",\n",
        "        \"short_answer\": \"9\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\",\n",
        "        \"cot_answer\": \"There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is 29.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\\"\\\"\\\"\\n    toys_initial = 5\\n    mom_toys = 2\\n    dad_toys = 2\\n    total_received = mom_toys + dad_toys\\n    total_toys = toys_initial + total_received\\n    result = total_toys\\n    return result\",\n",
        "        \"short_answer\": \"29\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\",\n",
        "        \"cot_answer\": \"Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is 33.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\\\"\\\"\\\"\\n    golf_balls_initial = 58\\n    golf_balls_lost_tuesday = 23\\n    golf_balls_lost_wednesday = 2\\n    golf_balls_left = golf_balls_initial - golf_balls_lost_tuesday - golf_balls_lost_wednesday\\n    result = golf_balls_left\\n    return result\",\n",
        "        \"short_answer\": \"33\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\",\n",
        "        \"cot_answer\": \"Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. So the answer is 8.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\\\"\\\"\\\"\\n    money_initial = 23\\n    bagels = 5\\n    bagel_cost = 3\\n    money_spent = bagels * bagel_cost\\n    money_left = money_initial - money_spent\\n    result = money_left\\n    return result\",\n",
        "        \"short_answer\": \"8\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUW8EO_yLRnx"
      },
      "outputs": [],
      "source": [
        "class GSM8KEvaluator(Evaluator):\n",
        "    def load_data(self):\n",
        "        ds = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
        "        examples = [{\"question\": example[\"question\"], \"answer\": example[\"answer\"].split(\"####\")[1].strip()} for example in ds]\n",
        "        for example in examples:\n",
        "            example[\"answer\"] = re.sub(r\"(\\d),(\\d)\", r\"\\1\\2\", example[\"answer\"])\n",
        "        return examples[:30]\n",
        "\n",
        "    def build_prompts(self, dataset, n_shot=8, demos=GSM_EXAMPLARS):\n",
        "        pass\n",
        "\n",
        "    def postprocess_output(self, output: str) -> str:\n",
        "        pass\n",
        "\n",
        "    def calculate_metrics(self, predictions, dataset):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrMv-1TxwQAt"
      },
      "outputs": [],
      "source": [
        "class GSM8KCoTEvaluator(GSM8KEvaluator):\n",
        "    def build_prompts(self, dataset, n_shot=8, demos=GSM_EXAMPLARS):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFYb2AcvwSHK"
      },
      "outputs": [],
      "source": [
        "cot_evaluator = GSM8KCoTEvaluator(llm)\n",
        "cot_evaluator.evaluate(n_shot=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSBQb38Vr4ir"
      },
      "source": [
        "## 1.4 Impact of Quantity on Few-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFsPcYfAr7mf"
      },
      "outputs": [],
      "source": [
        "cot_evaluator.evaluate(n_shot=1)\n",
        "cot_evaluator.evaluate(n_shot=2)\n",
        "cot_evaluator.evaluate(n_shot=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd1y7VFXr7xU"
      },
      "source": [
        "## 1.5 Retrieval Augmented Few-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLz37y9Uvtax"
      },
      "outputs": [],
      "source": [
        "class GSM8KRetrievalICLEvaluator(GSM8KEvaluator):\n",
        "    def build_prompts(self, dataset, n_shot=8, demos=GSM_EXAMPLARS):\n",
        "        \"\"\"Build prompts with RETRIVED_EXAMPLES we generated in 1.2.\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgrNhlc0r-uP"
      },
      "outputs": [],
      "source": [
        "retrieval_icl_evaluator = GSM8KRetrievalICLEvaluator(llm)\n",
        "retrieval_icl_evaluator.evaluate(n_shot=1)\n",
        "retrieval_icl_evaluator.evaluate(n_shot=2)\n",
        "retrieval_icl_evaluator.evaluate(n_shot=4)\n",
        "retrieval_icl_evaluator.evaluate(n_shot=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "035HhMvGsCNk"
      },
      "outputs": [],
      "source": [
        "# Rearrange the examples in ascending order of relevance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8sA91mdsCXh"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "- Q1.1: Regarding the contextual embedding in Section 1.1, how does sentiment classification performance compare to your word2vec results in A1? Discuss potential reasons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEyg3H3AK-0t"
      },
      "source": [
        "- Q1.2: In Section 1.4, which discusses the impact of quantity, what trends do you notice when adding contextual examples? Do you think this trend will continue?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtlFusQ-LAe4"
      },
      "source": [
        "- Q1.3: In Section 1.5, which covers retrieval-augmented in-context learning, how does this differ from Section 1.4? Analyze the reasons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0VMFubILBUf"
      },
      "source": [
        "- Q1.4: In Section 1.5, which arrangement yields better performance: in-context examples organized in descending or ascending order of relevance? Discuss the scenario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfwAdXU4urVp"
      },
      "source": [
        "# 2. Basic Decoding Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUc1HaH0J1M4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7WLNZn_9d7n"
      },
      "outputs": [],
      "source": [
        "\"\"\"set device and random seeds\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper functions are given to you.\n",
        "######################################################\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'device: {device}')\n",
        "\n",
        "def set_seed(seed=19260817):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJrHDSAIJ7ku"
      },
      "outputs": [],
      "source": [
        "\"\"\"load datasets\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('Ximing/ROCStories')\n",
        "train_data, dev_data, test_data = dataset['train'], dataset['validation'], dataset['test']\n",
        "\n",
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H4QKWbtJ9MW"
      },
      "outputs": [],
      "source": [
        "\"\"\"prepare evaluation\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from evaluate import load\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "\n",
        "perplexity_scorer = load(\"perplexity\", module_type=\"metric\")\n",
        "cola_model_name = \"textattack/roberta-base-CoLA\"\n",
        "cola_tokenizer = RobertaTokenizer.from_pretrained(cola_model_name)\n",
        "cola_model = RobertaForSequenceClassification.from_pretrained(cola_model_name).to(device)\n",
        "\n",
        "def batchify(data, batch_size):\n",
        "    assert batch_size > 0\n",
        "\n",
        "    batch = []\n",
        "    for item in data:\n",
        "        # Yield next batch\n",
        "        if len(batch) == batch_size:\n",
        "            yield batch\n",
        "            batch = []\n",
        "\n",
        "        batch.append(item)\n",
        "\n",
        "    # Yield last un-filled batch\n",
        "    if len(batch) != 0:\n",
        "        yield batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUuU65RsJ_GI"
      },
      "outputs": [],
      "source": [
        "\"\"\"set up evaluation metric\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "def compute_perplexity(texts, model='gpt2', batch_size=8):\n",
        "    score = perplexity_scorer.compute(predictions=texts, add_start_token=True, batch_size=batch_size, model_id=model)\n",
        "    return score['mean_perplexity']\n",
        "\n",
        "\n",
        "def compute_fluency(texts, batch_size=8):\n",
        "  scores = []\n",
        "  for b_texts in batchify(texts, batch_size):\n",
        "    inputs = cola_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "      logits = cola_model(**inputs).logits\n",
        "      probs = logits.softmax(dim=-1)\n",
        "      scores.extend(probs[:, 1].tolist())\n",
        "  return sum(scores) / len(scores)\n",
        "\n",
        "\n",
        "def compute_diversity(texts):\n",
        "    unigrams, bigrams, trigrams = [], [], []\n",
        "    total_words = 0\n",
        "    for gen in texts:\n",
        "        o = gen.split(' ')\n",
        "        total_words += len(o)\n",
        "        for i in range(len(o)):\n",
        "            unigrams.append(o[i])\n",
        "        for i in range(len(o) - 1):\n",
        "            bigrams.append(o[i] + '_' + o[i + 1])\n",
        "        for i in range(len(o) - 2):\n",
        "            trigrams.append(o[i] + '_' + o[i + 1] + '_' + o[i + 2])\n",
        "    return len(set(unigrams)) / len(unigrams), len(set(bigrams)) / len(bigrams), len(set(trigrams)) / len(trigrams)\n",
        "\n",
        "\n",
        "def evaluate(generations, experiment):\n",
        "  generations = [_ for _ in generations if _ != '']\n",
        "  perplexity = compute_perplexity(generations)\n",
        "  fluency = compute_fluency(generations)\n",
        "  diversity = compute_diversity(generations)\n",
        "  print(experiment)\n",
        "  print(f'perplexity = {perplexity:.2f}')\n",
        "  print(f'fluency = {fluency:.2f}')\n",
        "  print(f'diversity = {diversity[0]:.2f}, {diversity[1]:.2f}, {diversity[2]:.2f}')\n",
        "  print()\n",
        "\n",
        "debug_sents = [\"This restaurant is awesome\", \"My dog is cute and I love it.\", \"Today is sunny.\"]\n",
        "evaluate(debug_sents, 'debugging run')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxU5PdFtKBJO"
      },
      "outputs": [],
      "source": [
        "\"\"\"load model and tokenizer\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token=\"<|endoftext|>\")\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zXVylNcKFOJ"
      },
      "source": [
        "In this section, you will implement a few basic decoding algorithms:\n",
        "1. Greedy decoding\n",
        "2. Vanilla sampling\n",
        "3. Temperature sampling\n",
        "5. Top-p sampling\n",
        "\n",
        "We have provided a wrapper function `decode()` that takes care of batching, controlling max length, and handling the EOS token.\n",
        "You will be asked to implement the core function of each method: *given the pre-softmax logits of the next token, decide what the next token is.*\n",
        "\n",
        "**The wrapper calls the core function of each decoding algorithm, which you will implement in the subsections below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO8x8XJaKEwF"
      },
      "outputs": [],
      "source": [
        "\"\"\"decode main wrapper function\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "def decode(prompts, max_len, method, **kwargs):\n",
        "  encodings_dict = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "  input_ids = encodings_dict['input_ids'].to(device)\n",
        "  attention_mask = encodings_dict['attention_mask'].to(device)\n",
        "\n",
        "  model_kwargs = {'attention_mask': attention_mask}\n",
        "  batch_size, input_seq_len = input_ids.shape\n",
        "\n",
        "  unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=device)\n",
        "\n",
        "  for step in range(max_len):\n",
        "    model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**model_inputs, return_dict=True, output_attentions=False, output_hidden_states=False)\n",
        "\n",
        "    if step == 0:\n",
        "      last_non_masked_idx = torch.sum(attention_mask, dim=1) - 1\n",
        "      next_token_logits = outputs.logits[range(batch_size), last_non_masked_idx, :]\n",
        "    else:\n",
        "      next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "    log_prob = F.log_softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    if method == 'greedy':\n",
        "      next_tokens = greedy(next_token_logits)\n",
        "    elif method == 'sample':\n",
        "      next_tokens = sample(next_token_logits)\n",
        "    elif method == 'temperature':\n",
        "      next_tokens = temperature(next_token_logits, t=kwargs.get('t', 0.8))\n",
        "    elif method == 'topk':\n",
        "      next_tokens = topk(next_token_logits, k=kwargs.get('k', 20))\n",
        "    elif method == 'topp':\n",
        "      next_tokens = topp(next_token_logits, p=kwargs.get('p', 0.7))\n",
        "\n",
        "    # finished sentences should have their next token be a padding token\n",
        "    next_tokens = next_tokens * unfinished_sequences + tokenizer.pad_token_id * (1 - unfinished_sequences)\n",
        "\n",
        "    input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
        "    model_kwargs = model._update_model_kwargs_for_generation(outputs, model_kwargs, is_encoder_decoder=model.config.is_encoder_decoder)\n",
        "\n",
        "    # if eos_token was found in one sentence, set sentence to finished\n",
        "    unfinished_sequences = unfinished_sequences.mul((next_tokens != tokenizer.eos_token_id).long())\n",
        "\n",
        "    if unfinished_sequences.max() == 0:\n",
        "      break\n",
        "\n",
        "  response_ids = input_ids[:, input_seq_len:]\n",
        "  response_text = [tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True) for output in response_ids]\n",
        "\n",
        "  return response_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c17DrQISKNzI"
      },
      "outputs": [],
      "source": [
        "\"\"\"debug helper code\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "# For debugging, we duplicate a single prompt 10 times so that we obtain 10 generations for the same prompt\n",
        "dev_prompts = [dev_data[0]['prompt']] * 10\n",
        "\n",
        "def print_generations(prompts, generations):\n",
        "  for prompt, generation in zip(prompts, generations):\n",
        "    print(f'{[prompt]} ==> {[generation]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ryFGrlRSXYn"
      },
      "source": [
        "## 2.1 Greedy Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH0wBhy2SZa-"
      },
      "outputs": [],
      "source": [
        "def greedy(next_token_logits):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: compute `next_tokens` from `next_token_logits`.\n",
        "  # Hint: use torch.argmax()\n",
        "  next_tokens =\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "generations = decode(dev_prompts, max_len=20, method='greedy')\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGKL_31VJNw1"
      },
      "source": [
        "## 2.2 Vanilla Sampling and Temperature Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjrTLKj2JR5b"
      },
      "outputs": [],
      "source": [
        "def sample(next_token_logits):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: compute the probabilities `probs` from the logits.\n",
        "  # Hint: `probs` should have size (B, V)\n",
        "  probs =\n",
        "\n",
        "  # TODO: compute `next_tokens` from `probs`.\n",
        "  # Hint: use torch.multinomial()\n",
        "  next_tokens =\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='sample')\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25Su03uzJb_Z"
      },
      "outputs": [],
      "source": [
        "def temperature(next_token_logits, t):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - t: float\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: compute the probabilities `probs` from the logits, with temperature applied.\n",
        "  probs =\n",
        "\n",
        "  # TODO: compute `next_tokens` from `probs`.\n",
        "  next_tokens =\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='temperature', t=0.8)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSjMWNFEy_cC"
      },
      "source": [
        "## 2.3 Top-p Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq1ZwVVxzApa"
      },
      "outputs": [],
      "source": [
        "def topp(next_token_logits, p):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - p: float\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: Sort the logits in descending order, and compute\n",
        "  # the cumulative probabilities `cum_probs` on the sorted logits\n",
        "  sorted_logits, sorted_indices =\n",
        "  sorted_probs =\n",
        "  cum_probs =\n",
        "\n",
        "  # Create a mask to zero out all logits not in top-p\n",
        "  sorted_indices_to_remove = cum_probs > p\n",
        "  sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "  sorted_indices_to_remove[:, 0] = 0\n",
        "  # Restore mask to original indices\n",
        "  indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "\n",
        "  # Mask the logits\n",
        "  next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "  # TODO: Sample from the masked logits\n",
        "  probs =\n",
        "  next_tokens =\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='topp', p=0.7)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGfhvyy0Ka8i"
      },
      "source": [
        "## 2.4: Evaluation\n",
        "\n",
        "Run the following cell to obtain the evaluation results, which you should include in your writeup.\n",
        "Also don't forget to answer the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGyyCSgOKbgw"
      },
      "outputs": [],
      "source": [
        "prompts = [item['prompt'] for item in test_data][:10]\n",
        "GENERATIONS_PER_PROMPT = 10\n",
        "MAX_LEN = 100\n",
        "\n",
        "for experiment in ['greedy', 'sample', 'temperature', 'topp']:\n",
        "  generations = []\n",
        "  for prompt in tqdm(prompts):\n",
        "    generations += decode([prompt] * GENERATIONS_PER_PROMPT, max_len=MAX_LEN, method=experiment)\n",
        "  evaluate(generations, experiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUc_IncHKkRF"
      },
      "source": [
        "## Discussion\n",
        "- Q2.1: In greedy decoding, what do you observe when generating 10 times from the test prompt?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoDRS10iKzzc"
      },
      "source": [
        "- Q2.2: In vanilla sampling, what do you observe when generating 10 times from the test prompt?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghPXsI0YK1Db"
      },
      "source": [
        "- Q2.3: In temperature sampling, play around with the value of temperature $t$. Which value of $t$ makes it equivalent to greedy decoding? Which value of $t$ makes it equivalent to vanilla sampling?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE16fo91K2VM"
      },
      "source": [
        "- Q2.4: In top-$p$ sampling, play around with the value of $p$. Which value of $p$ makes it equivalent to greedy decoding? Which value of $p$ makes it equivalent to vanilla sampling?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wMu-w_FK3z5"
      },
      "source": [
        "- Q2.5: Report the evaluation metrics (perplexity, fluency, diversity) of all 4 decoding methods. Which methods have the best and worst perplexity? Fluency? Diversity?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.1 ('bert_chinese')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "53212d6bd219dd71a1dd1974b202da0e24447591239b9cfd3b7797526bde5b81"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
