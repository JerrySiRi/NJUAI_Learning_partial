{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4A-T0z5AU6"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.txt** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **1. clean all outputs and 2. run all cells in your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEomoMzH5Nf6"
      },
      "source": [
        "# 1 $n$-gram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsSAtTqt7Q8a"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data/lm\n",
        "!wget -O data/lm/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/train.txt\n",
        "!wget -O data/lm/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/dev.txt\n",
        "!wget -O data/lm/test.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ElrINWW7oF7"
      },
      "source": [
        "## 1.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eawcuVV19kZm"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "q-rNT_QL8Dvt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocabulary size is 26601\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "description: Building Vocabulary\n",
        "\"\"\"\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "V_uni = defaultdict(int) # set default number 0 to all tokens\n",
        "V_bi = defaultdict(int) # set default number 0 to all tokens\n",
        "N_train = 0 # number of tokens\n",
        "num_sentences = 0\n",
        "\n",
        "def preprocess(vo_dict, wh_filter=1):\n",
        "    voca_list = list(vo_dict.items())\n",
        "    # TODO: use np.where to implement filter, instead of for\n",
        "    sum_unk = 0\n",
        "    if wh_filter == 1:\n",
        "        voca_key, voca_values = zip(*voca_list)\n",
        "        index = np.where(np.array(voca_values)>=3)\n",
        "        valid_list = list(np.array(voca_list)[index])\n",
        "\n",
        "        invalid_index = np.where(np.array(voca_values)<3)\n",
        "        invalid_list = list(np.array(voca_list)[invalid_index])\n",
        "        invalid_list = [int(i[1]) for i in invalid_list]\n",
        "        sum_unk = sum(invalid_list)\n",
        "    else:\n",
        "        valid_list = voca_list\n",
        "    valid_list.append(('<UNK>', sum_unk))\n",
        "    valid_list.append(('<START>', num_sentences)) #【可以改成句子个数】\n",
        "    valid_list.append(('<END>', num_sentences))\n",
        "    return valid_list\n",
        "\n",
        "\n",
        "with open(\"./data/lm/train.txt\",'r+',encoding='utf-8') as f:\n",
        "    for line in f.readlines():\n",
        "        curline = line.strip().split(\" \") # use function:strip to drop \"\\n\"\n",
        "        for item in curline: # 1.compute c(w)\n",
        "            V_uni[item] += 1\n",
        "        num_sentences += 1\n",
        "# c(w) -- convert Vocabulary into different formats(list\\ndarray\\dict)\n",
        "uni_list = preprocess(V_uni, wh_filter=1)\n",
        "uni_array = np.array(uni_list)\n",
        "uni_dict = dict(uni_list)\n",
        "values = uni_dict.values()\n",
        "N_train = sum([int(i) for i in values ]) \n",
        "print(\"vocabulary size is\",len(uni_list))\n",
        "\n",
        "\n",
        "\n",
        "with open(\"./data/lm/train.txt\",'r+',encoding='utf-8') as f:\n",
        "    for line in f.readlines():\n",
        "        curline = line.strip().split(\" \") # use function:strip to drop \"\\n\"\n",
        "        for index in range(0, len(curline)):# 2.compute c(w,u)\n",
        "            if curline[index] not in uni_dict.keys(): # should be unk\n",
        "                curline[index] = '<UNK>'\n",
        "\n",
        "            if index == 0:\n",
        "                string = curline[index]+\"|\"+'<START>'\n",
        "            elif index != len(curline)-1:\n",
        "                if curline[index+1] not in uni_dict.keys(): # should be unk\n",
        "                    curline[index+1] = '<UNK>'\n",
        "                string = curline[index+1] + \"|\" + curline[index] \n",
        "                # contaminate two string, with shape \"new|old\"\n",
        "            else:\n",
        "                string = '<END>'+ '|' + curline[index] # w=<END>\n",
        "\n",
        "            V_bi[string] += 1\n",
        "\n",
        "# c(w,u) -- adjacent tokens pair\n",
        "bi_list = preprocess(V_bi, wh_filter=0)\n",
        "bi_array = np.array(bi_list)\n",
        "bi_dict = dict(bi_list)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7oBATsX8uHb"
      },
      "source": [
        "### Discussion：\n",
        "discuss the number of parameters of n-gram models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6VpAkS9odh"
      },
      "source": [
        "TBD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ2BGUig8TqH"
      },
      "source": [
        "## 1.2 $n$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeyANMPe9ad_"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "ACSfNZGE8Yw2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0106502804332709\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "description: build bigram and unigram language models and\n",
        "report their perplexity on the training set, and dev set\n",
        "\"\"\"\n",
        "import copy\n",
        "import math\n",
        "\n",
        "\n",
        "class N_Gram():\n",
        "    def __init__(self, indicator, smoothing=None, k=None) -> None:\n",
        "        self.indicator = indicator # which model: unigram or bigram\n",
        "        self.smoothing = smoothing # which smoothing method to be selected\n",
        "        self.k = k # add-k smoothing\n",
        "        self.uni_model = None\n",
        "        self.bi_model = None\n",
        "\n",
        "    def train(self):\n",
        "        if self.smoothing == None:\n",
        "            return self.no_smoothing() # return a dictionary of unigram or bigram \n",
        "        elif self.smoothing == \"add-one\":\n",
        "            pass\n",
        "        elif self.smoothing == \"add-k\":\n",
        "            pass\n",
        "        \n",
        "    def no_smoothing(self):\n",
        "\n",
        "        if self.indicator == \"unigram\": # model: {token: probability}\n",
        "            self.uni_model = copy.deepcopy(uni_dict) # frequency\n",
        "            for (token, fre) in uni_dict.items():\n",
        "                pro = int(fre) / N_train\n",
        "                self.uni_model[token] = pro # \"probability\"\n",
        "            return self.uni_model\n",
        "            \n",
        "        elif self.indicator == \"bigram\": # model: {token+1|token: probability}\n",
        "            self.bi_model = copy.deepcopy(bi_dict)\n",
        "            for (token, fre) in bi_dict.items():\n",
        "                sep_index = token.find('|')\n",
        "                last_token = token[sep_index+1 : ]\n",
        "                \n",
        "                if fre == 0: # c(w,u)=0\n",
        "                    pro = 0\n",
        "                elif last_token not in uni_dict.keys(): # frenquency<3\n",
        "                    last_token = '<UNK>'\n",
        "                    pro = int(fre) / int(uni_dict[last_token])\n",
        "                else:\n",
        "                    pro = int(fre) / int(uni_dict[last_token])\n",
        "                self.bi_model[token] = pro # \"probability\"\n",
        "            return self.bi_model\n",
        "            \n",
        "    def Perplexity(self,file=\"train\"):\n",
        "        if file == \"train\":\n",
        "            path = \"./data/lm/train.txt\"\n",
        "        elif file == \"dev\":\n",
        "            path = \"./data/lm/dev.txt\"\n",
        "        \n",
        "        Likelihood = 0\n",
        "        with open(path,\"r+\",encoding=\"utf-8\") as f:\n",
        "            M = 0\n",
        "            for line in f.readlines():\n",
        "                curline = line.strip().split(\" \") \n",
        "                multiply = 1\n",
        "                for index in range(0,len(curline)):\n",
        "                    M += len(curline)\n",
        "                    if self.indicator == \"unigram\":\n",
        "                        if curline[index] not in uni_dict.keys():\n",
        "                           curline[index] = '<UNK>' \n",
        "                        # 【此时，unigram永远不会是0！！！】\n",
        "                        # 【如果验证集、测试集中，遇到了没有遇到过的token（不是frequency<3）应如何处理】\n",
        "                        # 【feasible：记录一个UNK真实的列表，如果验证集的在，就用频率。不在，就设成0】\n",
        "                        multiply *= uni_dict[curline[index]]\n",
        "\n",
        "                    elif self.indicator == \"bigram\":\n",
        "                        # preprocess: convert to UNK\n",
        "                        if curline[index] not in uni_dict.keys():\n",
        "                            curline[index] = '<UNK>'\n",
        "                        # generate string\n",
        "                        if index == 0:\n",
        "                            string = curline[index]+'|'+'<START>'                    \n",
        "                        elif index == len(curline)-1:\n",
        "                            string = '<END>'+'|'+curline[index]\n",
        "                        else:\n",
        "                            if curline[index+1] not in uni_dict.keys():\n",
        "                                curline[index+1] = '<UNK>'\n",
        "                            string = curline[index+1]+'|'+curline[index]\n",
        "\n",
        "                        if string not in self.bi_model.keys(): multiply = 0; break\n",
        "                        else: multiply *= self.bi_model[string]\n",
        "                if multiply == 0: # exists unseen bi-pairs\n",
        "                    continue\n",
        "                Likelihood += math.log(multiply) \n",
        "            Likelihood *= 1/M\n",
        "        return 2**(-Likelihood) \n",
        "            \n",
        "\"\"\"\n",
        "test_1 = N_Gram(indicator='unigram', smoothing=None)\n",
        "uni_model = test_1.train()\n",
        "for i in range(200):\n",
        "    print(list(uni_model.items())[i])\n",
        "\"\"\"\n",
        "test_1 = N_Gram(indicator='bigram', smoothing=None)\n",
        "uni_model = test_1.train()\n",
        "ple = test_1.Perplexity(file=\"dev\")\n",
        "print(ple)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWC56a19TbY"
      },
      "source": [
        "### Discussion： \n",
        "If you encounter any problems, please analyze them and explain why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM4gcgL--Ylh"
      },
      "source": [
        "1. 由于删除了部分token（频率<3），部分算出来概率>1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuLL8CH1Ua-3"
      },
      "source": [
        "## 1.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7mWQhaCUixZ"
      },
      "source": [
        "### 1.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbHxLDmVrz6"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93_yLu9dVr0C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y1WOQtsVr0D"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTknh9pRVr0D"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTC0qJE8VVha"
      },
      "source": [
        "#### 1.3.2 Add-k smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3itGMOOVuNg"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhcuJWo7VuNg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzpU_p6CVuNg"
      },
      "source": [
        "##### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIOUpNXYVuNh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFWxsN-Uj0Y"
      },
      "source": [
        "### 1.3.3 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk11EpboWVCH"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4N_XuN6WVCQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh8v2P36WVCQ"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGkf6IV0WVCQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvgTZcNFVato"
      },
      "source": [
        "##### 1.3.4 Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKnXu98hWcfu"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKo4ZLASWcfu"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2 Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16844\\3803626181.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwv_from_bin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mwv_from_bin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_embedding_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16844\\3803626181.py\u001b[0m in \u001b[0;36mload_embedding_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mwv_from_bin\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAll\u001b[0m \u001b[1;36m400000\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meach\u001b[0m \u001b[0mlengh\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mwv_from_bin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"glove-wiki-gigaword-200\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loaded vocab size %i\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv_from_bin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
          ]
        }
      ],
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 400000 embeddings, each lengh 50\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
        "    return wv_from_bin\n",
        "\n",
        "wv_from_bin = load_embedding_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Find most similar word\n",
        "Use cosine similarity to find the most similar word to each of these words. Report the most similar word and its cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Finding Analogies\n",
        "Use vector addition and subtraction to compute target vectors for the analogies below. After computing each target vector, find the top three candidates by cosine similarity. Report the candidates and their similarities to the target vector.\n",
        "\n",
        "- dog : puppy :: cat : ?:\n",
        "- speak : speaker :: sing : ?:\n",
        "- France : French :: England : ?:\n",
        "- France : wine :: England : ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3 Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p data/classification\n",
        "!wget -O data/classification/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/train.txt\n",
        "!wget -O data/classification/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/dev.txt\n",
        "!wget -O data/classification/test-blind.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/test-blind.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unigram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bigram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GloVe Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare the performance of three types of features on dev set. Report the weighted average precision, recall and F1-score for each feature set.\n",
        "\n",
        "| Feature | precision | recall | F1-score |\n",
        "| ----------- | --------- | ------ | -------- |\n",
        "| unigram     |           |        |          |\n",
        "| bigram      |           |        |          |\n",
        "| GloVe       |           |        |          |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Better Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.1 ('com_torch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "2af8302a16cf6ed8ea0b8ddec60eac960f581ee85f49e07f3f8a491c7fcd9fb4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
