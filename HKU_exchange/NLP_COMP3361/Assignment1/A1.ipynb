{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4A-T0z5AU6"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.txt** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **1. clean all outputs and 2. run all cells in your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEomoMzH5Nf6"
      },
      "source": [
        "# 1 $n$-gram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsSAtTqt7Q8a"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data/lm\n",
        "!wget -O data/lm/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/train.txt\n",
        "!wget -O data/lm/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/dev.txt\n",
        "!wget -O data/lm/test.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ElrINWW7oF7"
      },
      "source": [
        "## 1.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eawcuVV19kZm"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "q-rNT_QL8Dvt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocabulary size is 26601\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "description: Building Vocabulary\n",
        "\"\"\"\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "V_uni = defaultdict(int) # set default number 0 to all tokens\n",
        "V_bi = defaultdict(int) # set default number 0 to all tokens\n",
        "V_tri = defaultdict(int) # set default number 0 to all tokens\n",
        "N_train = 0 # number of tokens\n",
        "num_sentences = 0\n",
        "\n",
        "def preprocess(vo_dict):\n",
        "    voca_list = list(vo_dict.items())\n",
        "    # TODO: use np.where to filter, instead of for\n",
        "    sum_unk = 0\n",
        "    voca_key, voca_values = zip(*voca_list)\n",
        "    index = np.where(np.array(voca_values)>=3)\n",
        "    valid_list = list(np.array(voca_list)[index])\n",
        "\n",
        "    invalid_index = np.where(np.array(voca_values)<3)\n",
        "    invalid_list = list(np.array(voca_list)[invalid_index])\n",
        "    invalid_list = [int(i[1]) for i in invalid_list]\n",
        "    sum_unk = sum(invalid_list)\n",
        "    valid_list.append(('<UNK>', sum_unk))\n",
        "    valid_list.append(('<START>', num_sentences)) #【可以改成句子个数】\n",
        "    valid_list.append(('<END>', num_sentences))\n",
        "    return valid_list\n",
        "\n",
        "# c(w) -- convert Vocabulary into different formats(list\\ndarray\\dict)\n",
        "with open(\"./data/lm/train.txt\",'r+',encoding='utf-8') as f:\n",
        "    for line in f.readlines():\n",
        "        curline = line.strip().split(\" \") # use function:strip to drop \"\\n\"\n",
        "        for item in curline: # 1.compute c(w)\n",
        "            V_uni[item] += 1\n",
        "        num_sentences += 1\n",
        "\n",
        "uni_list = preprocess(V_uni)\n",
        "uni_array = np.array(uni_list)\n",
        "uni_dict = dict(uni_list)\n",
        "values = uni_dict.values()\n",
        "N_train = sum([int(i) for i in values ]) \n",
        "print(\"vocabulary size is\",len(uni_list))\n",
        "\n",
        "\n",
        "# c(w,u)\\c(w,u,v) -- adjacent tokens pair\\tri-pair\n",
        "with open(\"./data/lm/train.txt\",'r+',encoding='utf-8') as f:\n",
        "    for line in f.readlines():\n",
        "        curline = line.strip().split(\" \") # use function:strip to drop \"\\n\"\n",
        "        for index in range(0, len(curline)+1):# 2.compute c(w,u)\\c(w,u,v)\n",
        "            if (index!=len(curline)) and (curline[index] not in uni_dict.keys()): # should be unk\n",
        "                curline[index] = '<UNK>'\n",
        "            string_bi = None\n",
        "            string_tri = None\n",
        "            if index == 0:\n",
        "                string_bi = curline[index]+\"|\"+'<START>'\n",
        "                string_tri = curline[index]+\"||\"+'<START>'+\"|\"+'<START>'\n",
        "            elif index == 1:\n",
        "                string_bi = curline[index] + '|'+curline[index-1]\n",
        "                string_tri = curline[index]+'||'+curline[index-1]+'|'+'<START>'\n",
        "            elif index == len(curline):\n",
        "                string_bi = '<END>'+'|'+curline[index-1]\n",
        "                string_tri = '<END>'+'||'+curline[index-1]+'|'+curline[index-2]\n",
        "                # contaminate two string, with shape \"new|old\"\n",
        "            else:\n",
        "                string_bi = curline[index] + '|' + curline[index-1]\n",
        "                string_tri = curline[index] + \"||\" + curline[index-1] + \"|\" + curline[index-2] \n",
        "\n",
        "            V_bi[string_bi] += 1\n",
        "            V_tri[string_tri] += 1\n",
        "\n",
        "bi_list = list(V_bi.items())\n",
        "bi_array = np.array(bi_list)\n",
        "bi_dict = dict(bi_list)\n",
        "\n",
        "tri_list = list(V_tri.items())\n",
        "tri_array = np.array(tri_list)\n",
        "tri_dict = dict(tri_list)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7oBATsX8uHb"
      },
      "source": [
        "### Discussion：\n",
        "discuss the number of parameters of n-gram models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6VpAkS9odh"
      },
      "source": [
        "TBD\n",
        "\n",
        "1. 可以对所有大小写统一，统一成小写（26601-》24106）。Section 3完成\n",
        "2. 需要对由于UNK（次数<3次就计入UNK）、START\\END（每一句话都加一个START、END）所导致的特殊字符频率过高的问题，。进行讨论》》分析 or 可视化 概率，来看特殊字符的影响！\n",
        "3. 在每一节的discussion的位置给出解决问题的描述 & 结果分析【用不用第三方库都是可以的】"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ2BGUig8TqH"
      },
      "source": [
        "## 1.2 $n$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeyANMPe9ad_"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assumption:\n",
        "1. unigram perplexity $$\\frac{1}{M}\\sum_{i=1}^M \\sqrt[N_i]{\\prod_{i=1}^{N_i}{\\frac{1}{p(w_i)}} }  $$\n",
        "2. bigram perplexity $$\\frac{1}{M}\\sum_{i=1}^M \\sqrt[N_i]{\\prod_{i=1}^{N_i}{\\frac{1}{p(w_i|w_{i-1})}} }  $$\n",
        "3. trigram perplexity $$\\frac{1}{M}\\sum_{i=1}^M \\sqrt[N_i]{\\prod_{i=1}^{N_i}{\\frac{1}{p(w_i|w_{i-1},w_{i-2})}} }  $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "ACSfNZGE8Yw2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1247.5862002554384\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "description: build bigram and unigram language models and\n",
        "report their perplexity on the training set, and dev set\n",
        "\"\"\"\n",
        "import copy\n",
        "import math\n",
        "\n",
        "\n",
        "class N_Gram():\n",
        "    def __init__(self, indicator, smoothing=None, k=None) -> None:\n",
        "        self.indicator = indicator # which model: unigram or bigram or trigram\n",
        "        self.smoothing = smoothing # which smoothing method to be selected\n",
        "        self.k = k # add-k smoothing\n",
        "        self.uni_model = None\n",
        "        self.bi_model = None\n",
        "        self.tri_model = None\n",
        "\n",
        "    def train(self):\n",
        "        if self.smoothing == None:\n",
        "            return self.no_smoothing() # return a dictionary of unigram or bigram \n",
        "        elif self.smoothing == \"add-one\":\n",
        "            pass\n",
        "        elif self.smoothing == \"add-k\":\n",
        "            pass\n",
        "        \n",
        "    def no_smoothing(self):\n",
        "        if self.indicator == \"unigram\": # model: {token: probability}\n",
        "            self.uni_model = copy.deepcopy(uni_dict) # frequency\n",
        "            for (token, fre) in uni_dict.items():\n",
        "                pro = int(fre) / N_train\n",
        "                self.uni_model[token] = pro # \"probability\"\n",
        "            return self.uni_model\n",
        "            \n",
        "        elif self.indicator == \"bigram\": # model: {token|token-1: probability}\n",
        "            self.bi_model = copy.deepcopy(bi_dict)\n",
        "            for (token, fre) in bi_dict.items():\n",
        "                sep_index = token.find('|')\n",
        "                last_token = token[sep_index+1 : ]\n",
        "                if fre == 0: # c(w,u)=0\n",
        "                    pro = 0\n",
        "                elif last_token not in uni_dict.keys(): # frenquency<3\n",
        "                    last_token = '<UNK>'\n",
        "                    pro = int(fre) / int(uni_dict[last_token])\n",
        "                else:\n",
        "                    pro = int(fre) / int(uni_dict[last_token])\n",
        "                self.bi_model[token] = pro # \"probability\"\n",
        "            return self.bi_model\n",
        "\n",
        "        elif self.indicator == \"trigram\": # model: {token||token-1|token-2: probability}\n",
        "            self.tri_model = copy.deepcopy(tri_dict)\n",
        "            for (token, fre) in tri_dict.items():\n",
        "                sep_index = token.find('||')\n",
        "                last_two_token = token[sep_index+1 : ]\n",
        "                if fre == 0:\n",
        "                    pro = 0\n",
        "                elif last_two_token not in bi_dict.keys():\n",
        "                    last_two_token = '<UNK>|<UNK>'\n",
        "                    pro = int(fre) / int(bi_dict[last_two_token])\n",
        "                else:\n",
        "                    pro = int(fre) / int(bi_dict[last_two_token])\n",
        "                self.tri_model[token] = pro # \"probability\"\n",
        "            return self.tri_model\n",
        "            \n",
        "    def Perplexity(self,file=\"train\"):\n",
        "        if file == \"train\":\n",
        "            path = \"./data/lm/train.txt\"\n",
        "        elif file == \"dev\":\n",
        "            path = \"./data/lm/dev.txt\"\n",
        "        elif file == \"test\":\n",
        "            path = \"./data/lm/test.txt\"\n",
        "        \n",
        "        per_sum = 0\n",
        "        with open(path,\"r+\",encoding=\"utf-8\") as f:\n",
        "            M = 0\n",
        "            for line in f.readlines():\n",
        "                M += 1\n",
        "                curline = line.strip().split(\" \") \n",
        "                multiply = 1\n",
        "                for index in range(0,len(curline)+1):\n",
        "                    if self.indicator == \"unigram\":\n",
        "                        if index == len(curline):\n",
        "                            continue\n",
        "                        if curline[index] not in self.uni_model.keys():\n",
        "                           curline[index] = '<UNK>' \n",
        "                        # 【此时，unigram永远不会是0！！！】\n",
        "                        # 【如果验证集、测试集中，遇到了没有遇到过的token（不是frequency<3）应如何处理】\n",
        "                        # A:验证集 or 测试集如果遇到了没有遇到的，同样应给成<UNK>!![而不是给成0！]\n",
        "                        multiply *= self.uni_model[curline[index]]\n",
        "\n",
        "                    elif self.indicator == \"bigram\":\n",
        "                        # preprocess: convert to UNK\n",
        "                        if index != len(curline) and curline[index] not in uni_dict.keys():\n",
        "                            curline[index] = '<UNK>'\n",
        "                        # generate string\n",
        "                        if index == 0:\n",
        "                            string = curline[index]+'|'+'<START>'                    \n",
        "                        elif index == len(curline):\n",
        "                            string = '<END>'+'|'+curline[index-1]\n",
        "                        else:\n",
        "                            string = curline[index]+'|'+curline[index-1]\n",
        "                        if string not in self.bi_model.keys(): \n",
        "                            multiply = 0\n",
        "                            M -= 1 \n",
        "                            break\n",
        "                        else: \n",
        "                            multiply *= self.bi_model[string]\n",
        "                    \n",
        "                    elif self.indicator == \"trigram\":\n",
        "                        if index != len(curline) and curline[index] not in uni_dict.keys():\n",
        "                            curline[index] = '<UNK>'\n",
        "                        if index == 0:\n",
        "                            string = curline[index]+'||'+'<START>'+'|'+'<START>'   \n",
        "                        elif index == 1:\n",
        "                            string = curline[index]+'||'+curline[index-1]+'|'+'<START>'\n",
        "                        elif index == len(curline):\n",
        "                            string = '<END>'+'||'+ curline[index-1]+'|'+curline[index-2]\n",
        "                        else: \n",
        "                            string = curline[index]+'||'+curline[index-1]+'|'+curline[index-2]\n",
        "                        if string not in self.tri_model.keys(): \n",
        "                            multiply = 0\n",
        "                            M -= 1 \n",
        "                            break\n",
        "                        else: \n",
        "                            multiply *= self.tri_model[string]\n",
        "\n",
        "                if multiply == 0: # exists unseen bi-pairs\n",
        "                    continue\n",
        "                token_value = math.pow(1/multiply,1/len(curline))\n",
        "                if token_value < 10000: # longlonglong sentences\n",
        "                    per_sum += token_value\n",
        "                # print(\"multiply\",multiply,\"len\",len(curline),\"value\",token_value)\n",
        "            return (1/M)*per_sum\n",
        "            \n",
        "\"\"\"\n",
        "test_1 = N_Gram(indicator='unigram', smoothing=None)\n",
        "uni_model = test_1.train()\n",
        "for i in range(200):\n",
        "    print(list(uni_model.items())[i])\n",
        "\"\"\"\n",
        "test_1 = N_Gram(indicator='unigram', smoothing=None)\n",
        "tri_model = test_1.train()\n",
        "ple = test_1.Perplexity(file=\"dev\")\n",
        "\n",
        "print(ple)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWC56a19TbY"
      },
      "source": [
        "### Discussion： \n",
        "If you encounter any problems, please analyze them and explain why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM4gcgL--Ylh"
      },
      "source": [
        "1. 由于删除了部分token（频率<3），部分算出来概率>1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuLL8CH1Ua-3"
      },
      "source": [
        "## 1.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7mWQhaCUixZ"
      },
      "source": [
        "### 1.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbHxLDmVrz6"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93_yLu9dVr0C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y1WOQtsVr0D"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTknh9pRVr0D"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTC0qJE8VVha"
      },
      "source": [
        "#### 1.3.2 Add-k smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3itGMOOVuNg"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhcuJWo7VuNg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzpU_p6CVuNg"
      },
      "source": [
        "##### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIOUpNXYVuNh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFWxsN-Uj0Y"
      },
      "source": [
        "### 1.3.3 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk11EpboWVCH"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4N_XuN6WVCQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh8v2P36WVCQ"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGkf6IV0WVCQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvgTZcNFVato"
      },
      "source": [
        "##### 1.3.4 Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKnXu98hWcfu"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKo4ZLASWcfu"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2 Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15380\\3803626181.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwv_from_bin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mwv_from_bin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_embedding_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15380\\3803626181.py\u001b[0m in \u001b[0;36mload_embedding_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mwv_from_bin\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAll\u001b[0m \u001b[1;36m400000\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meach\u001b[0m \u001b[0mlengh\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mwv_from_bin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"glove-wiki-gigaword-200\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loaded vocab size %i\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv_from_bin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
          ]
        }
      ],
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 400000 embeddings, each lengh 50\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
        "    return wv_from_bin\n",
        "\n",
        "wv_from_bin = load_embedding_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Find most similar word\n",
        "Use cosine similarity to find the most similar word to each of these words. Report the most similar word and its cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Finding Analogies\n",
        "Use vector addition and subtraction to compute target vectors for the analogies below. After computing each target vector, find the top three candidates by cosine similarity. Report the candidates and their similarities to the target vector.\n",
        "\n",
        "- dog : puppy :: cat : ?:\n",
        "- speak : speaker :: sing : ?:\n",
        "- France : French :: England : ?:\n",
        "- France : wine :: England : ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3 Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p data/classification\n",
        "!wget -O data/classification/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/train.txt\n",
        "!wget -O data/classification/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/dev.txt\n",
        "!wget -O data/classification/test-blind.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/test-blind.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unigram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bigram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GloVe Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare the performance of three types of features on dev set. Report the weighted average precision, recall and F1-score for each feature set.\n",
        "\n",
        "| Feature | precision | recall | F1-score |\n",
        "| ----------- | --------- | ------ | -------- |\n",
        "| unigram     |           |        |          |\n",
        "| bigram      |           |        |          |\n",
        "| GloVe       |           |        |          |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Better Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.1 ('com_torch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "2af8302a16cf6ed8ea0b8ddec60eac960f581ee85f49e07f3f8a491c7fcd9fb4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
