{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4A-T0z5AU6"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.txt** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **1. clean all outputs and 2. run all cells in your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEomoMzH5Nf6"
      },
      "source": [
        "# 1 $n$-gram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsSAtTqt7Q8a"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data/lm\n",
        "!wget -O data/lm/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/train.txt\n",
        "!wget -O data/lm/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/dev.txt\n",
        "!wget -O data/lm/test.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ElrINWW7oF7"
      },
      "source": [
        "## 1.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eawcuVV19kZm"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q-rNT_QL8Dvt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocabulary size is 26601\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "description: Building Vocabulary\n",
        "\"\"\"\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "V_uni = defaultdict(int) # set default number 0 to all tokens\n",
        "V_bi = defaultdict(int) # set default number 0 to all tokens\n",
        "V_tri = defaultdict(int) # set default number 0 to all tokens\n",
        "N_train = 0 # number of tokens\n",
        "num_sentences = 0 # num of sentences in training data (number of <START> and <END> at the same time)\n",
        "train_corpus_amount = 0\n",
        "\n",
        "def preprocess(vo_dict): # filter process -> <UNK>\n",
        "    # TODO: use np.where to filter, instead of \"for\"\n",
        "    voca_list = list(vo_dict.items())  \n",
        "    sum_unk = 0\n",
        "    voca_key, voca_values = zip(*voca_list)\n",
        "    index = np.where(np.array(voca_values)>=3)\n",
        "    valid_list = list(np.array(voca_list)[index])\n",
        "\n",
        "    invalid_index = np.where(np.array(voca_values)<3)\n",
        "    invalid_list = list(np.array(voca_list)[invalid_index])\n",
        "    invalid_list = [int(i[1]) for i in invalid_list]\n",
        "    sum_unk = sum(invalid_list)\n",
        "    valid_list.append(('<UNK>', sum_unk))\n",
        "    valid_list.append(('<START>', num_sentences)) # bi-gram will use it\n",
        "    valid_list.append(('<END>', num_sentences))\n",
        "    return valid_list\n",
        "\n",
        "# c(w) -- convert Vocabulary into different formats(list\\ndarray\\dict)\n",
        "with open(\"./data/lm/train.txt\",'r+',encoding='utf-8') as f:\n",
        "    for line in f.readlines():\n",
        "        curline = line.strip().split(\" \") # use function \"strip\" to drop \"\\n\"\n",
        "        for item in curline: \n",
        "            V_uni[item] += 1\n",
        "        num_sentences += 1\n",
        "\n",
        "uni_list = preprocess(V_uni)\n",
        "uni_dict = dict(uni_list)\n",
        "\n",
        "values = uni_dict.values()\n",
        "N_train = sum([int(i) for i in values ]) \n",
        "N_train -= 2*num_sentences # token number in training data, without <START> and <END>\n",
        "train_corpus_amount = len(uni_list)\n",
        "print(\"vocabulary size is\", train_corpus_amount)\n",
        "\n",
        "\n",
        "# c(w,u)\\c(w,u,v) -- adjacent tokens pair\\tri-pair\n",
        "with open(\"./data/lm/train.txt\",'r+',encoding='utf-8') as f:\n",
        "    for line in f.readlines():\n",
        "        curline = line.strip().split(\" \") # use function \"strip\" to drop \"\\n\"\n",
        "        for index in range(0, len(curline)+1):\n",
        "            if (index!=len(curline)) and (curline[index] not in uni_dict.keys()): # should be <UNK>\n",
        "                curline[index] = '<UNK>'\n",
        "            \n",
        "            string_bi = None\n",
        "            string_tri = None\n",
        "            if index == 0:\n",
        "                string_bi = curline[index]+\"|\"+'<START>'\n",
        "                string_tri = curline[index]+\"||\"+'<START>'+\"|\"+'<START>'\n",
        "            elif index == 1:\n",
        "                string_bi = curline[index] + '|'+curline[index-1]\n",
        "                string_tri = curline[index]+'||'+curline[index-1]+'|'+'<START>'\n",
        "            elif index == len(curline):\n",
        "                string_bi = '<END>'+'|'+curline[index-1]\n",
        "                string_tri = '<END>'+'||'+curline[index-1]+'|'+curline[index-2]\n",
        "            else:\n",
        "                string_bi = curline[index] + '|' + curline[index-1]\n",
        "                string_tri = curline[index] + \"||\" + curline[index-1] + \"|\" + curline[index-2] \n",
        "            V_bi[string_bi] += 1\n",
        "            V_tri[string_tri] += 1\n",
        "\n",
        "temp_bi = list(V_bi.items())\n",
        "temp_bi.append(('<START>|<START>', num_sentences)) # tri-gram will use it\n",
        "temp_bi.append(('<START>|<START>', num_sentences))\n",
        "bi_list = temp_bi\n",
        "bi_dict = dict(bi_list)\n",
        "\n",
        "tri_list = list(V_tri.items())\n",
        "tri_dict = dict(tri_list)\n",
        "\n",
        "# Tag: uni\\bi\\tri_dict ------ frequency (counting number)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7oBATsX8uHb"
      },
      "source": [
        "### Discussion：\n",
        "discuss the number of parameters of n-gram models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6VpAkS9odh"
      },
      "source": [
        "TBD\n",
        "\n",
        "1. 可以对所有大小写统一，统一成小写（26601-》24106）。Section 3完成\n",
        "2. 需要对由于UNK（次数<3次就计入UNK）、START\\END（每一句话都加一个START、END）所导致的特殊字符频率过高的问题进行讨论》》分析 or 可视化 概率，来看特殊字符的影响！\n",
        "3. 在每一节的discussion的位置给出解决问题的描述 & 结果分析【用不用第三方库都是可以的】"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ2BGUig8TqH"
      },
      "source": [
        "## 1.2 $n$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeyANMPe9ad_"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assumption:\n",
        "1. unigram perplexity $$\\frac{1}{M}\\sum_{i=1}^M \\sqrt[N_i]{\\prod_{i=1}^{N_i}{\\frac{1}{p(w_i)}} }  $$\n",
        "2. bigram perplexity $$\\frac{1}{M}\\sum_{i=1}^M \\sqrt[N_i]{\\prod_{i=1}^{N_i}{\\frac{1}{p(w_i|w_{i-1})}} }  $$\n",
        "3. trigram perplexity $$\\frac{1}{M}\\sum_{i=1}^M \\sqrt[N_i]{\\prod_{i=1}^{N_i}{\\frac{1}{p(w_i|w_{i-1},w_{i-2})}} }  $$\n",
        "   \n",
        "tip: Perplexities in Section 1 are not taken into account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ACSfNZGE8Yw2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: unigram model's perplexity with no smoothing method is 1261.1489758884582\n",
            "Train: bigram model's perplexity with no smoothing method is 98.14200160020923\n",
            "Train: trigram model's perplexity with no smoothing method is 9.829401146863853\n",
            "\n",
            "\n",
            "Dev: unigram model's perplexity with no smoothing method is 1158.1941706978978\n",
            "Dev: bigram model's perplexity with no smoothing method is 83.82548692125181\n",
            "Dev: trigram model's perplexity with no smoothing method is 32.5206511202772\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import math\n",
        "\n",
        "class N_Gram():\n",
        "    \"\"\"\n",
        "    Description: uni/bi/tri-gram model \n",
        "    Parameters: \n",
        "        - indicator: indicate the type of current object \n",
        "            [feasible values: unigram, bigram, trigram, linear_interpolation]\n",
        "        - smoothing: smoothing method\n",
        "            [feasible values: None, add-one, add-k, linear_interpolation]\n",
        "        - k: hyperparameter when use add-k smoothing\n",
        "        - uni_model: probability of each uni-words\n",
        "        - bi_model:  probability of each bi-pairs\n",
        "        - tri_model: probability of each tri-pairs\n",
        "        - l_i_model: probability of each tri-pairs in linear_interpolation model\n",
        "    \"\"\"\n",
        "    def __init__(self, indicator, smoothing=None, k=None) -> None:\n",
        "        self.indicator = indicator # which model: unigram or bigram or trigram\n",
        "        self.smoothing = smoothing # which smoothing method to be selected\n",
        "        self.k = k # add-k smoothing\n",
        "        self.uni_model = None\n",
        "        self.bi_model = None\n",
        "        self.tri_model = None\n",
        "        self.l_i_model = None\n",
        "        \"\"\"self.train_in = 0\n",
        "        self.dev_in = 0\n",
        "        self.train_notin = 0\n",
        "        self.dev_notin = 0\"\"\"\n",
        "\n",
        "# Tag: self.uni\\bi\\tri_model ------ probability\n",
        "\n",
        "    def train(self):\n",
        "        if self.smoothing == None:\n",
        "            return self.no_smoothing_training() # return a dictionary of unigram or bigram \n",
        "        elif self.smoothing == \"add-one\":\n",
        "            return self.add_one_training()\n",
        "        elif self.smoothing == \"add-k\":\n",
        "            return self.add_k_training()\n",
        "        elif self.smoothing == 'linear_interpolation':\n",
        "            return self.linear_interpolation_training()\n",
        "        \n",
        "    def no_smoothing_training(self):\n",
        "        if self.indicator == \"unigram\": # model: {token: probability}\n",
        "            self.uni_model = copy.deepcopy(uni_dict) # frequency\n",
        "            for (token, fre) in uni_dict.items():\n",
        "                pro = int(fre) / N_train\n",
        "                self.uni_model[token] = pro # \"probability\"\n",
        "            return self.uni_model\n",
        "            \n",
        "        elif self.indicator == \"bigram\": # model: {token|token-1: probability}\n",
        "            self.bi_model = copy.deepcopy(bi_dict)\n",
        "            for (token, fre) in bi_dict.items():\n",
        "                sep_index = token.find('|')\n",
        "                last_token = token[sep_index+1 : ]\n",
        "                if last_token not in uni_dict.keys(): \n",
        "                    pro = int(fre) / int(uni_dict['<UNK>'])\n",
        "                else:\n",
        "                    pro = int(fre) / int(uni_dict[last_token])\n",
        "                self.bi_model[token] = pro # \"probability\"\n",
        "            return self.bi_model\n",
        "\n",
        "        elif self.indicator == \"trigram\": # model: {token||token-1|token-2: probability}\n",
        "            self.tri_model = copy.deepcopy(tri_dict)\n",
        "            for (token, fre) in tri_dict.items():\n",
        "                sep_index = token.find('||')\n",
        "                last_two_token = token[sep_index+2 : ]\n",
        "                # BUG: \"find\" function find the first index of \"||\"\n",
        "                if last_two_token not in bi_dict.keys():\n",
        "                    pro = int(fre) / int(bi_dict['<UNK>|<UNK>'])\n",
        "                else:\n",
        "                    pro = int(fre) / int(bi_dict[last_two_token])\n",
        "                self.tri_model[token] = pro # \"probability\"\n",
        "            return self.tri_model\n",
        "            \n",
        "    def Perplexity(self,file=\"train\"):\n",
        "        name_path = {\"train\":\"./data/lm/train.txt\", \"dev\":\"./data/lm/dev.txt\", \"test\":\"./data/lm/test.txt\"}\n",
        "        path = name_path[file]\n",
        "\n",
        "        per_sum = 0\n",
        "        with open(path,\"r+\",encoding=\"utf-8\") as f:\n",
        "            sen_num = 0\n",
        "            for line in f.readlines():\n",
        "                sen_num += 1\n",
        "                curline = line.strip().split(\" \") \n",
        "                multiply = 1\n",
        "                for index in range(0,len(curline)+1):\n",
        "                    if self.indicator == \"unigram\":\n",
        "                        if index == len(curline):\n",
        "                            continue\n",
        "                        if curline[index] not in self.uni_model.keys():\n",
        "                           curline[index] = '<UNK>' \n",
        "                        # 【此时，unigram永远不会是0！！！】\n",
        "                        # 【如果验证集、测试集中，遇到了没有遇到过的token（不是frequency<3）应如何处理】\n",
        "                        # A:验证集 or 测试集如果遇到了没有遇到的，同样应给成<UNK>!![而不是给成0！]\n",
        "                        multiply *= self.uni_model[curline[index]]\n",
        "\n",
        "\n",
        "\n",
        "                    elif self.indicator == \"bigram\":\n",
        "                        # preprocess: convert to UNK\n",
        "                        later_token = ''\n",
        "                        if index != len(curline) and curline[index] not in uni_dict.keys():\n",
        "                            curline[index] = '<UNK>'\n",
        "                        # generate string\n",
        "                        if index == 0:\n",
        "                            string = curline[index]+'|'+'<START>'  \n",
        "                            later_token = '<START>'              \n",
        "                        elif index == len(curline):\n",
        "                            string = '<END>'+'|'+curline[index-1]\n",
        "                            later_token = curline[index-1]\n",
        "                        else:\n",
        "                            string = curline[index]+'|'+curline[index-1]\n",
        "                            later_token = curline[index-1]\n",
        "                        \n",
        "                        if self.smoothing == None: # un-seen token (do not assign probability to it)\n",
        "                            if string not in self.bi_model.keys(): \n",
        "                                multiply = 0\n",
        "                                sen_num -= 1 \n",
        "                                break\n",
        "                            else: \n",
        "                                multiply *= self.bi_model[string]\n",
        "                            \n",
        "                        elif self.smoothing == \"add-one\":\n",
        "                            if string not in self.bi_model.keys(): \n",
        "                                multiply = int(bi_dict['<UNK>|<UNK>']) + 1 / (int(uni_dict[later_token]) + train_corpus_amount)\n",
        "                                \"\"\"if file == \"train\":\n",
        "                                    self.train_notin += 1\n",
        "                                elif file == \"dev\":\n",
        "                                    self.dev_notin += 1\"\"\"\n",
        "                            else: \n",
        "                                multiply *= self.bi_model[string]\n",
        "                                \"\"\"if file == \"train\":\n",
        "                                    self.train_in += 1\n",
        "                                elif file == \"dev\":\n",
        "                                    self.dev_in += 1\"\"\"\n",
        "\n",
        "                        elif self.smoothing == \"add-k\":\n",
        "                            if string not in self.bi_model.keys(): \n",
        "                                multiply = int(bi_dict['<UNK>|<UNK>']) + self.k / (int(uni_dict[later_token]) + self.k*train_corpus_amount)\n",
        "                            else: \n",
        "                                multiply *= self.bi_model[string]\n",
        "                    \n",
        "\n",
        "\n",
        "                    elif self.indicator == \"trigram\" or self.indicator == \"linear_interpolation\":\n",
        "                        if self.indicator == \"trigram\":\n",
        "                            current_model = self.tri_model\n",
        "                        elif self.indicator == \"linear_interpolation\":\n",
        "                            current_model = self.l_i_model\n",
        "\n",
        "                        if index != len(curline) and curline[index] not in uni_dict.keys():\n",
        "                            curline[index] = '<UNK>'\n",
        "                        if index == 0:\n",
        "                            string = curline[index]+'||'+'<START>'+'|'+'<START>'   \n",
        "                        elif index == 1:\n",
        "                            string = curline[index]+'||'+curline[index-1]+'|'+'<START>'\n",
        "                        elif index == len(curline):\n",
        "                            string = '<END>'+'||'+ curline[index-1]+'|'+curline[index-2]\n",
        "                        else: \n",
        "                            string = curline[index]+'||'+curline[index-1]+'|'+curline[index-2]\n",
        "\n",
        "                        if string not in current_model.keys(): \n",
        "                            multiply = 0\n",
        "                            sen_num -= 1 \n",
        "                            break\n",
        "                        else: \n",
        "                            multiply *= current_model[string]\n",
        "              \n",
        "\n",
        "                if multiply == 0: # exists unseen bi\\tri-pairs\n",
        "                    continue\n",
        "                token_value = math.pow(1/multiply,1/len(curline))\n",
        "                if token_value < 10000: # longlonglong sentences\n",
        "                    per_sum += token_value\n",
        "                # print(\"multiply\",multiply,\"len\",len(curline),\"value\",token_value)\n",
        "            #print(\"train_in=\",self.train_in,\"train_notin=\",self.train_notin,\n",
        "                #\"dev_in=\",self.dev_in,\"dev_notin=\",self.dev_notin,)\n",
        "            return (1/sen_num)*per_sum\n",
        "\n",
        "    def add_one_training(self):\n",
        "        return \"undefined add_one model yet\"\n",
        "    \n",
        "    def add_k_training(self):\n",
        "        return \"undefined add_k model yet\"\n",
        "    \n",
        "    def linear_interpolation_training(self):\n",
        "        return \"undefined linear_interpolation model yet\"\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "# Training Phase\n",
        "u_m = N_Gram(indicator='unigram', smoothing=None)\n",
        "b_m = N_Gram(indicator='bigram', smoothing=None)\n",
        "t_m = N_Gram(indicator='trigram', smoothing=None)\n",
        "u_t = u_m.train()\n",
        "b_t = b_m.train()\n",
        "t_t = t_m.train()\n",
        "\n",
        "u_ple_train = u_m.Perplexity(file=\"train\")\n",
        "b_ple_train = b_m.Perplexity(file=\"train\")\n",
        "t_ple_train = t_m.Perplexity(file=\"train\")\n",
        "\n",
        "\n",
        "# Testing Phase\n",
        "print(\"Train: unigram model's perplexity with no smoothing method is {0}\".format(u_ple_train))\n",
        "print(\"Train: bigram model's perplexity with no smoothing method is {0}\".format(b_ple_train))\n",
        "print(\"Train: trigram model's perplexity with no smoothing method is {0}\".format(t_ple_train))\n",
        "print('\\n')\n",
        "u_ple_dev = u_m.Perplexity(file=\"dev\")\n",
        "b_ple_dev = b_m.Perplexity(file=\"dev\")\n",
        "t_ple_dev = t_m.Perplexity(file=\"dev\")\n",
        "print(\"Dev: unigram model's perplexity with no smoothing method is {0}\".format(u_ple_dev))\n",
        "print(\"Dev: bigram model's perplexity with no smoothing method is {0}\".format(b_ple_dev))\n",
        "print(\"Dev: trigram model's perplexity with no smoothing method is {0}\".format(t_ple_dev))\n",
        "\n",
        "# TODO；不同数据集对效果的影响！\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWC56a19TbY"
      },
      "source": [
        "### Discussion： \n",
        "1. discuss the experimental results\n",
        "2. If you encounter any problems, please analyze them and explain why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM4gcgL--Ylh"
      },
      "source": [
        "1. 由于删除了部分token（频率<3），部分算出来概率>1\n",
        "2. 困惑度不满足unigram > bigram > trigram的原因，可能是由于没有smoothing，一句话里只要有没见过的二元、三元组，bigram、trigram就给丢掉【可能这句话很有把握，但是就一个词没有见到，就被舍弃】"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuLL8CH1Ua-3"
      },
      "source": [
        "## 1.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7mWQhaCUixZ"
      },
      "source": [
        "### 1.3.1 Add-one (Laplace) smoothing [training set and dev set]\n",
        "\n",
        "1. compare perplexity between bigram without smoothing and bigram with add-one smoothing\n",
        "\n",
        "2. report perplexity on training data and dev\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbHxLDmVrz6"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "93_yLu9dVr0C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: bigram model's perplexity with add-one smoothing method is 2225.6414136546873\n",
            "Dev: bigram model's perplexity with add-one smoothing method is 55.465280325390076\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class Bigram_add_one(N_Gram): # only implement add-one smoothing on bi-gram\n",
        "    def add_one_training(self):\n",
        "        if self.indicator != \"bigram\":\n",
        "            return super().add_one_training()\n",
        "        else:\n",
        "            self.bi_model = copy.deepcopy(bi_dict)\n",
        "            for (token, fre) in bi_dict.items():\n",
        "                sep_index = token.find('|')\n",
        "                last_token = token[sep_index+1 : ]\n",
        "                if last_token not in uni_dict.keys(): # frenquency < 3\n",
        "                    last_token = '<UNK>'\n",
        "\n",
        "                pro = (int(fre) + 1) / (int(uni_dict[last_token]) + train_corpus_amount)\n",
        "                self.bi_model[token] = pro # \"probability\"\n",
        "            return self.bi_model\n",
        "\n",
        "test_1 = Bigram_add_one(indicator='bigram', smoothing=\"add-one\")\n",
        "bi_t = test_1.train()\n",
        "ple_train = test_1.Perplexity(\"train\")\n",
        "ple_dev = test_1.Perplexity(\"dev\")\n",
        "print(\"Train: bigram model's perplexity with add-one smoothing method is {0}\".format(ple_train))\n",
        "print(\"Dev: bigram model's perplexity with add-one smoothing method is {0}\".format(ple_dev))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y1WOQtsVr0D"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTknh9pRVr0D"
      },
      "source": [
        "1. Q: 基本差距不大，可能是因为多乘了一个1/fre+V的概率和sentence个数多了一个能抵消？\n",
        "2. Q: bigram使用add-one和不使用add-one在训练集上性能差距过大，在验证集上性能几乎不变\n",
        "   A: train.txt中的bi-gram有1616746个，由于都在训练模型时各个bigram概率都使用add-one修正，在计算perplexity的时候的所有概率全部都被缩小\n",
        "   dev.txt中bi-gram一共有323816个，其中训练时得到的bi_dict覆盖了256528个，未覆盖67288个。相比train.txt中bigram个数量级差别较大，导致原来不使用smoothing method时两个困惑度基本相同，但在使用add-one后由于train.txt中pairs个数很多，就会多乘很多比原来小的概率，进而比dev.txt差很多啦！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTC0qJE8VVha"
      },
      "source": [
        "#### 1.3.2 Add-k smoothing [training and dev set]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3itGMOOVuNg"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jhcuJWo7VuNg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current value of k is: 0.5\n",
            "Train: bigram model's perplexity with add-k smoothing method is 1503.7395343096173\n",
            "Dev: bigram model's perplexity with add-k smoothing method is 36.74802184519655\n",
            "Current value of k is: 0.05\n",
            "Train: bigram model's perplexity with add-k smoothing method is 424.1169438774384\n",
            "Dev: bigram model's perplexity with add-k smoothing method is 12.948378779162084\n",
            "Current value of k is: 0.01\n",
            "Train: bigram model's perplexity with add-k smoothing method is 215.32164031501006\n",
            "Dev: bigram model's perplexity with add-k smoothing method is 8.256363243235695\n"
          ]
        }
      ],
      "source": [
        "class Bigram_add_k(N_Gram): # only implement add-one smoothing on bi-gra\n",
        "    def add_k_training(self):\n",
        "        if self.indicator != \"bigram\":\n",
        "            return super().add_one_training()\n",
        "        else:\n",
        "            self.bi_model = copy.deepcopy(bi_dict)\n",
        "            for (token, fre) in bi_dict.items():\n",
        "                sep_index = token.find('|')\n",
        "                last_token = token[sep_index+1 : ]\n",
        "                if last_token not in uni_dict.keys(): # frenquency < 3\n",
        "                    last_token = '<UNK>'\n",
        "\n",
        "                pro = (int(fre) + self.k) / (int(uni_dict[last_token]) + self.k * train_corpus_amount)\n",
        "                self.bi_model[token] = pro # \"probability\"\n",
        "            return self.bi_model\n",
        "k_list = [0.5, 0.05, 0.01]\n",
        "for cur_k in k_list:\n",
        "    test = Bigram_add_k(indicator='bigram', smoothing=\"add-k\", k=cur_k)\n",
        "    bi_t = test.train()\n",
        "    ple_train = test.Perplexity(\"train\")\n",
        "    ple_dev = test.Perplexity(\"dev\")\n",
        "    print(\"Current value of k is:\",cur_k)\n",
        "    print(\"Train: bigram model's perplexity with add-k smoothing method is {0}\".format(ple_train))\n",
        "    print(\"Dev: bigram model's perplexity with add-k smoothing method is {0}\".format(ple_dev))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzpU_p6CVuNg"
      },
      "source": [
        "##### Discussion\n",
        "\n",
        "Briefly discuss the differences between add-k models (3 different k) and add-one models [on bigram]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIOUpNXYVuNh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFWxsN-Uj0Y"
      },
      "source": [
        "### 1.3.3 Linear Interpolation [optimize on dev set, report perplexity on test set]\n",
        "\n",
        "1. Report its perplexity on the training set and dev set for the values $\\lambda_1$ = 0.2, $\\lambda_2$ = 0.3, $\\lambda_3$ = 0.5.\n",
        "2. Please optimize on a dev set by trying different hyperparameter sets. \n",
        "3. Finally, report the perplexity on the test set with the best hyperparameter set you get. Briefly discuss the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk11EpboWVCH"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "K4N_XuN6WVCQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- task 1: report perplexity on train & dev set with initial values of \\lambda_1_2_3 -----\n",
            "Train: linear_interpolation model's perplexity with linear_interpolation smoothing method is 16.06279614001182\n",
            "Dev: linear_interpolation model's perplexity with linear_interpolation smoothing method is 31.928149799280426\n",
            "\n",
            "\n",
            "----- task 2: find the best hyperparameter set, along with regarding perplexity -----\n",
            "the best hyperparameter of (lambda_1, lambda_2, lambda_3) is: (0.0, 0.3, 0.7) respectively\n",
            "the regarding perplexity with best hyperparameter set in dev.txt is: 26.600135677586284\n",
            "\n",
            "\n",
            "----- task 3: report perplexity on test set with the best values of \\lambda_1_2_3 -----\n",
            "Test: linear_interpolation model's perplexity with linear_interpolation smoothing method is 27.23544219591197\n"
          ]
        }
      ],
      "source": [
        "# --- task 1 ---\n",
        "lam_1 = 0.2\n",
        "lam_2 = 0.3\n",
        "lam_3 = 0.5\n",
        "\n",
        "\n",
        "class Linear_Interpolation(N_Gram): # only implement add-one smoothing on bi-gra\n",
        "    def linear_interpolation_training(self):\n",
        "        const_uni_model = None\n",
        "        const_bi_model = None\n",
        "        const_tri_model = None\n",
        "\n",
        "        models = [\"unigram\", \"bigram\", \"trigram\"]\n",
        "        for cur_type in models:\n",
        "            self.indicator = cur_type\n",
        "            if cur_type == \"unigram\": \n",
        "                temp = self.no_smoothing_training()\n",
        "                const_uni_model = copy.deepcopy(temp)\n",
        "            elif cur_type == \"bigram\": \n",
        "                temp = self.no_smoothing_training()\n",
        "                const_bi_model = copy.deepcopy(temp)\n",
        "            elif cur_type == \"trigram\": \n",
        "                temp = self.no_smoothing_training()\n",
        "                const_tri_model = copy.deepcopy(temp)\n",
        "        self.indicator = 'linear_interpolation'\n",
        "        self.l_i_model = copy.deepcopy(self.tri_model)\n",
        "\n",
        "        for (token, fre) in self.tri_model.items():\n",
        "            index_1 = token.find('||')\n",
        "            first = token[:index_1]\n",
        "            last_two = token[index_1+2:]\n",
        "            index_2 = last_two.find('|')\n",
        "            second = last_two[:index_2]\n",
        "            uni_string = first\n",
        "            bi_string = first + '|' +second\n",
        "            tri_string = token\n",
        "\n",
        "            pro_uni, pro_bi, pro_tri = 0, 0, 0\n",
        "            if uni_string not in self.uni_model.keys(): pro_uni = self.uni_model['<UNK>']\n",
        "            else:pro_uni = self.uni_model[uni_string]\n",
        "            if bi_string not in self.bi_model.keys():pro_bi = self.bi_model['<UNK>|<UNK>']\n",
        "            else:pro_bi = self.bi_model[bi_string]\n",
        "            if tri_string not in self.tri_model.keys(): pro_tri = self.tri_model['<UNK>||<UNK>|<UNK>']\n",
        "            else:pro_tri = self.tri_model[tri_string]\n",
        "        \n",
        "            pro = lam_1*pro_uni + lam_2*pro_bi + lam_3*pro_tri\n",
        "            self.l_i_model[token] = pro # \"probability\"\n",
        "        return self.l_i_model,const_uni_model,const_bi_model,const_tri_model\n",
        "\n",
        "l_i_model = Linear_Interpolation(indicator='linear_interpolation', smoothing='linear_interpolation')\n",
        "l_i,const_uni_model,const_bi_model,const_tri_model\\\n",
        "     = l_i_model.train()\n",
        "ple_train = l_i_model.Perplexity(\"train\")\n",
        "ple_dev = l_i_model.Perplexity(\"dev\")\n",
        "print('----- task 1: report perplexity on train & dev set with initial values of \\lambda_1_2_3 -----')\n",
        "print(\"Train: linear_interpolation model's perplexity with linear_interpolation smoothing method is {0}\".format(ple_train))\n",
        "print(\"Dev: linear_interpolation model's perplexity with linear_interpolation smoothing method is {0}\".format(ple_dev))\n",
        "\n",
        "\n",
        "# --- task 2 ---\n",
        "\n",
        "feasible_value = []\n",
        "for i in range(0,10):\n",
        "    lam_1 = i/10\n",
        "    for j in range(0,10-i+1):\n",
        "        lam_2 = j/10\n",
        "        lam_3 = (10-i-j)/10\n",
        "        feasible_value.append((lam_1,lam_2,lam_3)) # immutable element could be dictionary's key!!!\n",
        "\n",
        "\n",
        "class Optimize_linear_Interpolation(N_Gram): \n",
        "    def linear_interpolation_training(self):\n",
        "        feaisble_perplexity = {}.fromkeys(feasible_value,0)\n",
        "\n",
        "        self.l_i_model = copy.deepcopy(const_tri_model)\n",
        "        for (lam_1,lam_2,lam_3) in feasible_value:\n",
        "            for (token, fre) in const_tri_model.items():\n",
        "                index_1 = token.find('||')\n",
        "                first = token[:index_1]\n",
        "                last_two = token[index_1+2:]\n",
        "                index_2 = last_two.find('|')\n",
        "                second = last_two[:index_2]\n",
        "                uni_string = first\n",
        "                bi_string = first + '|' +second\n",
        "                tri_string = token\n",
        "\n",
        "                pro_uni, pro_bi, pro_tri = 0, 0, 0\n",
        "                if uni_string not in const_uni_model.keys(): pro_uni = const_uni_model['<UNK>']\n",
        "                else:pro_uni = const_uni_model[uni_string]\n",
        "                if bi_string not in const_bi_model.keys(): pro_bi = const_bi_model['<UNK>|<UNK>']\n",
        "                else:pro_bi = const_bi_model[bi_string]\n",
        "                if tri_string not in const_tri_model.keys(): pro_tri = const_tri_model['<UNK>||<UNK>|<UNK>']\n",
        "                else:pro_tri = const_tri_model[tri_string]\n",
        "                pro = lam_1*pro_uni + lam_2*pro_bi + lam_3*pro_tri\n",
        "                self.l_i_model[token] = pro # \"probability\"\n",
        "\n",
        "            cur_per = self.Perplexity(file='dev')\n",
        "            feaisble_perplexity[(lam_1,lam_2,lam_3)] = cur_per\n",
        "        return feaisble_perplexity # return a dict\n",
        "\n",
        "o_l_i_model = Optimize_linear_Interpolation(indicator='linear_interpolation', smoothing='linear_interpolation')\n",
        "result_dict = o_l_i_model.train()\n",
        "print('\\n')\n",
        "result_list = list(result_dict.items())\n",
        "def list_sort(item):\n",
        "    return item[1]\n",
        "result_list.sort(key=list_sort)\n",
        "print(\"----- task 2: find the best hyperparameter set, along with regarding perplexity -----\")\n",
        "print(\"the best hyperparameter of (lambda_1, lambda_2, lambda_3) is: {0} respectively\".format(result_list[0][0]))\n",
        "print(\"the regarding perplexity with best hyperparameter set in dev.txt is: {0}\".format(result_list[0][1]))\n",
        "\n",
        "\n",
        "# --- task 3 ---\n",
        "print('\\n')\n",
        "lam_1,lam_2,lam_3 = result_list[0][0]\n",
        "test_model = Linear_Interpolation(indicator='linear_interpolation', smoothing='linear_interpolation')\n",
        "l_i,const_uni_model,const_bi_model,const_tri_model\\\n",
        "     = test_model.train()\n",
        "ple_test = test_model.Perplexity(\"test\")\n",
        "print('----- task 3: report perplexity on test set with the best values of \\lambda_1_2_3 -----')\n",
        "print(\"Test: linear_interpolation model's perplexity with linear_interpolation smoothing method is {0}\".format(ple_test))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh8v2P36WVCQ"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGkf6IV0WVCQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvgTZcNFVato"
      },
      "source": [
        "##### 1.3.4 Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKnXu98hWcfu"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKo4ZLASWcfu"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2 Word Vectors [use pre-trained Glove]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded vocab size 400000\n"
          ]
        }
      ],
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 400000 embeddings, each lengh 50\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
        "    return wv_from_bin\n",
        "\n",
        "wv_from_bin = load_embedding_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Find most similar word\n",
        "Use cosine similarity to find the most similar word to each of these words. Report the most similar word and its cosine similarity.\n",
        "- dog\n",
        "- whale\n",
        "- before\n",
        "- however\n",
        "- fabricate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'gensim.models.keyedvectors.KeyedVectors'>\n",
            "word:dog        - most similar word:cat        - regarding cosine similarity is:0.7444875836372375        \n",
            "word:whale      - most similar word:humpback   - regarding cosine similarity is:0.702865719795227        \n",
            "word:before     - most similar word:when       - regarding cosine similarity is:0.8611732125282288        \n",
            "word:however    - most similar word:though     - regarding cosine similarity is:0.8910207152366638        \n",
            "word:fabricate  - most similar word:invent     - regarding cosine similarity is:0.5735598802566528        \n"
          ]
        }
      ],
      "source": [
        "# we have wv_from_bin \n",
        "# learning material: https://blog.csdn.net/u010700066/article/details/83070102\n",
        "\"\"\"\n",
        "print(wv_from_bin.get_vector(\"dog\"))  # 查看向量\n",
        "wv_from_bin.key_to_index  # 查看词和对应向量\n",
        "wv_from_bin.index2word  # 每个index对应的词\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(type(wv_from_bin))\n",
        "\n",
        "# --- task 1 --- \n",
        "vocabulary = ['dog', 'whale', 'before', 'however', 'fabricate']\n",
        "for item in vocabulary:\n",
        "    print('word:{0:<10} - most similar word:{1:<10} - regarding cosine similarity is:{2:<10}\\\n",
        "        '.format(item,wv_from_bin.most_similar(item)[1][0],wv_from_bin.most_similar(item)[1][1]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Finding Analogies\n",
        "Use vector addition and subtraction to compute target vectors for the analogies below. After computing each target vector, find the top three candidates by cosine similarity. Report the candidates and their similarities to the target vector.\n",
        "\n",
        "- dog : puppy :: cat : ?:\n",
        "- speak : speaker :: sing : ?:\n",
        "- france : french :: england : ?:\n",
        "- france : wine :: england : ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the potential answers of analogy \"dog:puppy::cat:?\" are [('puppy', 0.7710572481155396), ('cat', 0.6285784840583801), ('puppies', 0.6080518960952759)]\n",
            "\n",
            "\n",
            "the potential answers of analogy \"speak:speaker::sing:?\" are [('speaker', 0.6879462003707886), ('sing', 0.5925208926200867), ('sang', 0.5009135603904724)]\n",
            "\n",
            "\n",
            "the potential answers of analogy \"france:french::england:?\" are [('english', 0.7608821988105774), ('england', 0.759766697883606), ('british', 0.5845818519592285)]\n",
            "\n",
            "\n",
            "the potential answers of analogy \"france:wine::england:?\" are [('wine', 0.6376140713691711), ('tea', 0.5302954912185669), ('england', 0.5272895693778992)]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "vocabulary_list = [('dog',\"puppy\", \"cat\"), (\"speak\", \"speaker\", \"sing\"), ('france', \"french\", \"england\"),\\\n",
        "    (\"france\", \"wine\", \"england\")]\n",
        "\n",
        "for item in vocabulary_list:\n",
        "    start_exi = item[0]; A = wv_from_bin.get_vector(start_exi)\n",
        "    end_exi = item[1]; B = wv_from_bin.get_vector(end_exi)\n",
        "    start_new = item[2]; C = wv_from_bin.get_vector(start_new)\n",
        "    D = C + (B - A)\n",
        "    print('the potential answers of analogy \"{0}:{1}::{2}:?\" are {3}'.format(item[0],item[1],item[2],\\\n",
        "        wv_from_bin.most_similar(D)[:3] ))\n",
        "    print('\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3 Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p data/classification\n",
        "!wget -O data/classification/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/train.txt\n",
        "!wget -O data/classification/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/dev.txt\n",
        "!wget -O data/classification/test-blind.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/test-blind.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Logistic Regression\n",
        "1. build three types of features to train a classifier: Unigram, Bigram, and GloVe\n",
        "(Global Vectors for Word Representation) features. \n",
        "2. Use logistic regression to predict the sentiment labels of movie reviews. \n",
        "    You can only use train.txt to train the model. \n",
        "3. Report the P, R, F1 results of dev.txt in a table and compare the performance difference of these three features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data preprocess \n",
        "- generate unigram and bigram features dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocabulary size is 5067 with <UNK>, <START>, <END>\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "V_uni = defaultdict(int) # set default number 0 to all tokens\n",
        "V_bi = defaultdict(int) # set default number 0 to all tokens\n",
        "sentence_label = [] # labels in train.txt\n",
        "sentence_set = [] # elements of each line in train.txt\n",
        "corpus = [] # for tf-idf to optimize weights\n",
        "N_train = 0 # number of tokens [inclusive tokens which appear several times]\n",
        "num_sentences = 0 # num of sentences in training data (number of <START> and <END> at the same time)\n",
        "train_corpus_amount = 0 # number of tokens [exclusive tokens appear several times]\n",
        "\n",
        "def preprocess(vo_dict, filter=\"unigram\", lower_bound=3): # filter process -> <UNK>\n",
        "    # TODO: use np.where to filter, instead of \"for\"\n",
        "    voca_list = list(vo_dict.items())  \n",
        "    sum_unk = 0\n",
        "    voca_key, voca_values = zip(*voca_list)\n",
        "    index = np.where(np.array(voca_values)>=3)\n",
        "    valid_list = list(np.array(voca_list)[index])\n",
        "    invalid_index = np.where(np.array(voca_values)<3)\n",
        "    invalid_list = list(np.array(voca_list)[invalid_index])\n",
        "    invalid_list_num = [int(i[1]) for i in invalid_list]\n",
        "    sum_unk = sum(invalid_list_num)\n",
        "    if filter == \"unigram\":\n",
        "        valid_list.append(('<UNK>', sum_unk))\n",
        "    else:\n",
        "        valid_list.append(('<UNK>|<UNK>', sum_unk))\n",
        "    return valid_list,invalid_list\n",
        "\n",
        "# Unigram Features --- c(w): convert Vocabulary into different formats(list\\dict)\n",
        "with open(\"./data/classification/train.txt\",'r+',encoding='utf-8') as f:\n",
        "    for line in f.readlines():\n",
        "        curline = line.strip().split(\"\\t\")\n",
        "        sentence_label.append(int(curline[0]))\n",
        "        string = str(curline[1])\n",
        "        corpus.append(string)\n",
        "        curline = string.split(\" \")\n",
        "         # use function \"strip\" to drop \"\\n\"\n",
        "        sentence_set.append(curline)\n",
        "        for index in range(0,len(curline)): \n",
        "            V_uni[curline[index]] += 1\n",
        "        num_sentences += 1\n",
        "\n",
        "uni_fre_list, unk_fre_list = preprocess(V_uni,filter=\"unigram\",lower_bound=3)\n",
        "uni_fre_list.append(('<START>', num_sentences)) # bi-gram will use it\n",
        "uni_fre_list.append(('<END>', num_sentences))\n",
        "uni_fre_dict = dict(uni_fre_list)\n",
        "unk_fre_dict = dict(unk_fre_list)\n",
        "\n",
        "values = uni_fre_dict.values()\n",
        "N_train = sum([int(i) for i in values])\n",
        "N_train -= 2*num_sentences # token number in training data, without <START> and <END>\n",
        "train_corpus_amount = len(uni_fre_list)\n",
        "print(\"vocabulary size is\", train_corpus_amount,\"with <UNK>, <START>, <END>\")\n",
        "\n",
        "\n",
        "# Bigram Features: c(w,u) -- adjacent tokens pair (list\\dict)\n",
        "for curline in sentence_set:\n",
        "    for index in range(0, len(curline)+1):\n",
        "        if (index!=len(curline)) and (curline[index] not in uni_fre_dict.keys()): # should be <UNK>\n",
        "            curline[index] = '<UNK>'    \n",
        "        string_bi = None\n",
        "        if index == 0:\n",
        "            string_bi = curline[index]+\"|\"+'<START>'\n",
        "        elif index == 1:\n",
        "            string_bi = curline[index] + '|'+curline[index-1]\n",
        "        elif index == len(curline):\n",
        "            string_bi = '<END>'+'|'+curline[index-1]\n",
        "        else:\n",
        "            string_bi = curline[index] + '|' + curline[index-1]\n",
        "        V_bi[string_bi] += 1\n",
        "\n",
        "bi_fre_list, bi_unk_list = preprocess(V_bi,filter=\"bigram\",lower_bound=3)\n",
        "bi_fre_list.append(('<START>|<START>', num_sentences)) # tri-gram will use it\n",
        "bi_fre_list.append(('<START>|<START>', num_sentences))\n",
        "bi_fre_dict = dict(bi_fre_list)\n",
        "bi_unk_dict = dict(bi_unk_list)\n",
        "# Tag: uni\\bi\\tri_dict ------ frequency (counting number)\n",
        "\n",
        "\n",
        "# 可能去掉频率低的词汇会降低准确率？比如一些不常用的表达情感词语\n",
        "# 可能减少一些N_train可以提升准确率？\n",
        "# 可能用原来的频次可能会提升准确率？不除以所有词出现的次数？\n",
        "# 特征的归一化、标准化大概率可以提升性能"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unigram Features -- generate LR classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def build_unigram_feature(sentence_element, dimension = None):\n",
        "    uni_feature = []\n",
        "    keys =  uni_fre_dict.keys()\n",
        "    unk_list = unk_fre_dict.keys() # unk_list --- token need to be unknown \n",
        "    for line in sentence_element:\n",
        "        cur_dict = defaultdict(int) # frequency of each word occurs in this line\n",
        "        for item in line:\n",
        "            if item in unk_list:\n",
        "                cur_dict['<UNK>'] += 1\n",
        "            else:\n",
        "                cur_dict[item] += 1\n",
        "        length = len(line) # length of current line, inclusive <UNK>\n",
        "        current_embedding = [] \n",
        "        for item in keys:\n",
        "            if item in line: # vocabulary\n",
        "                current_embedding.append(cur_dict[item]/length)\n",
        "            else:\n",
        "                current_embedding.append(0)\n",
        "        uni_feature.append(current_embedding)\n",
        "        \n",
        "    # 处理1：标准化\n",
        "    scaler = StandardScaler() # standarize data\n",
        "    scaler = scaler.fit(uni_feature) # fit，本质是生成均值和方差\n",
        "    uni_feature = scaler.transform(uni_feature) #通过接口导出结果\n",
        "\n",
        "    # 处理2：降维\n",
        "    # n_components=500, score = 0.7903179190751445 0.7898843930635838\n",
        "    pca = PCA(n_components=dimension)\n",
        "    pca = pca.fit(uni_feature)\n",
        "    # print(pca.explained_variance_ratio_)\n",
        "    new_uni_features = pca.transform(uni_feature)\n",
        "    return new_uni_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bigram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def build_bi_feature(sentence_element, dimension = None):\n",
        "    bi_feature = []\n",
        "    known_bigram = bi_fre_dict.keys() # vocabulary_bigram \n",
        "    # unknown_bigram = bi_unk_dict.keys() # unknown_bigram --- bigram need to be unknown (<UNK>|<UNK>) \n",
        "    for line in sentence_element:\n",
        "        cur_dict = defaultdict(int)\n",
        "        for index in range(0,len(line)+1):\n",
        "            if index == 0: string = line[index] +'|'+ '<START>'\n",
        "            elif index == len(line): string = '<END>' +'|'+ line[index-1]\n",
        "            else: string  = line[index] +'|'+ line[index-1]\n",
        "\n",
        "            if string not in known_bigram: cur_dict['<UNK>|<UNK>'] += 1\n",
        "            else: cur_dict[string] += 1\n",
        "        \n",
        "        length = len(line)\n",
        "        current_embedding = [] \n",
        "        cur_keys = cur_dict.keys()\n",
        "        for item in known_bigram:\n",
        "            if item in cur_keys:\n",
        "                current_embedding.append(cur_dict[item]/length)\n",
        "            else:\n",
        "                current_embedding.append(0)\n",
        "        bi_feature.append(current_embedding)\n",
        "        \n",
        "    # 处理1：标准化\n",
        "    scaler = StandardScaler() # standarize data\n",
        "    scaler = scaler.fit(bi_feature) # fit，本质是生成均值和方差\n",
        "    bi_feature = scaler.transform(bi_feature) #通过接口导出结果\n",
        "\n",
        "    # 处理2：降维\n",
        "    # n_components=800, score = 0.7656069364161849 0.7582369942196532\n",
        "    pca = PCA(n_components=dimension)\n",
        "    pca = pca.fit(bi_feature)\n",
        "    # print(pca.explained_variance_ratio_)\n",
        "    new_bi_feature = pca.transform(bi_feature)\n",
        "    return new_bi_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GloVe Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "model = api.load('glove-wiki-gigaword-200')\n",
        "def build_Glove_feature(sentence_set):\n",
        "    tfidf = TfidfVectorizer()\n",
        "    tfidf.fit_transform(corpus)\n",
        "    feature = []\n",
        "    for sentence in sentence_set:\n",
        "        words = sentence\n",
        "        words = [word.lower() for word in words] # convert each token to lower type\n",
        "        word_embeddings = [model.get_vector(word) for word in words if word in model]\n",
        "    \n",
        "        index = [] # whether each word in current sentence has its embedding in Glove\n",
        "        for word in words:\n",
        "            if word in model: index.append(True)\n",
        "            else: index.append(False)\n",
        "\n",
        "        # 使用TF-IDF加权嵌入\n",
        "        weights = np.array([tfidf.idf_[tfidf.vocabulary_.get(word, 0)] for word in words])\n",
        "        weights = np.expand_dims(weights, axis=1)\n",
        "        weighted_embeddings = weights[index] * np.array(word_embeddings)\n",
        "        # 取所有单词嵌入的加权平均值\n",
        "        if len(weighted_embeddings) > 0:\n",
        "            sentence_embedding = np.mean(weighted_embeddings, axis=0)\n",
        "        else:\n",
        "            sentence_embedding = np.zeros(model.vector_size)\n",
        "        feature.append(sentence_embedding)\n",
        "    return feature\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train model with different features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram model 0.7087155963302753\n",
            "Bigram model 0.7752293577981652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "c:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "c:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "c:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "c:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "c:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "c:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "c:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "c:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "c:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "c:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Glove model 0.7981651376146789\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression as LR\n",
        "from sklearn.linear_model import LogisticRegressionCV as LRCV\n",
        "\n",
        "uni_dimension = 50\n",
        "bi_dimension = 200\n",
        "\n",
        "new_uni_features = build_unigram_feature(sentence_set, dimension = uni_dimension)\n",
        "new_bi_feature = build_bi_feature(sentence_set, dimension = bi_dimension)\n",
        "Glove_feature = build_Glove_feature(sentence_set)\n",
        "\n",
        "# TODO:可以把三个训练过程合在一起写成一个函数呢！\n",
        "# unigram --- train model\n",
        "# 处理3：预测时加入正则化项（如L1，进行特征选择）。也要尝试L2好不好\n",
        "uni_lrcvl1 = LRCV(penalty='l1',solver='saga', max_iter=2000)\n",
        "uni_lrcvl1 = uni_lrcvl1.fit(new_uni_features, sentence_label)\n",
        "uni_lrcv_score = uni_lrcvl1.score(new_uni_features, sentence_label)\n",
        "print(\"Unigram model\", uni_lrcv_score)\n",
        "\n",
        "# bigram --- train model\n",
        "bi_lrcvl1 = LRCV(penalty='l1',solver='saga', max_iter=2000)\n",
        "bi_lrcvl1 = bi_lrcvl1.fit(new_bi_feature, sentence_label)\n",
        "bi_lrcv_score = bi_lrcvl1.score(new_bi_feature, sentence_label)\n",
        "print(\"Bigram model\", bi_lrcv_score)\n",
        "\n",
        "# Glove --- train model\n",
        "Glove_lrcvl1 = LRCV(penalty='l1',solver='saga', max_iter=2000)\n",
        "Glove_lrcvl1 = Glove_lrcvl1.fit(Glove_feature, sentence_label)\n",
        "Glove_lrcv_score = Glove_lrcvl1.score(Glove_feature, sentence_label)\n",
        "print(\"Glove model\", Glove_lrcv_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build three features on dev set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocabulary size is 808 with <UNK>, <START>, <END>\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "V_uni = defaultdict(int) # set default number 0 to all tokens\n",
        "V_bi = defaultdict(int) # set default number 0 to all tokens\n",
        "sentence_label = [] # labels in train.txt\n",
        "sentence_set = [] # elements of each line in train.txt\n",
        "corpus = [] # for tf-idf to optimize weights\n",
        "N_train = 0 # number of tokens [inclusive tokens which appear several times]\n",
        "num_sentences = 0 # num of sentences in training data (number of <START> and <END> at the same time)\n",
        "train_corpus_amount = 0 # number of tokens [exclusive tokens appear several times]\n",
        "\n",
        "# Unigram Features --- c(w): convert Vocabulary into different formats(list\\dict)\n",
        "with open(\"./data/classification/dev.txt\",'r+',encoding='utf-8') as f:\n",
        "    for line in f.readlines():\n",
        "        curline = line.strip().split(\"\\t\")\n",
        "        sentence_label.append(int(curline[0]))\n",
        "        string = str(curline[1])\n",
        "        corpus.append(string)\n",
        "        curline = string.split(\" \")\n",
        "         # use function \"strip\" to drop \"\\n\"\n",
        "        sentence_set.append(curline)\n",
        "        for index in range(0,len(curline)): \n",
        "            V_uni[curline[index]] += 1\n",
        "        num_sentences += 1\n",
        "\n",
        "uni_fre_list, unk_fre_list = preprocess(V_uni,filter=\"unigram\",lower_bound=3)\n",
        "uni_fre_list.append(('<START>', num_sentences)) # bi-gram will use it\n",
        "uni_fre_list.append(('<END>', num_sentences))\n",
        "uni_fre_dict = dict(uni_fre_list)\n",
        "unk_fre_dict = dict(unk_fre_list)\n",
        "values = uni_fre_dict.values()\n",
        "\n",
        "N_train = sum([int(i) for i in values])\n",
        "N_train -= 2*num_sentences # token number in training data, without <START> and <END>\n",
        "train_corpus_amount = len(uni_fre_list)\n",
        "print(\"vocabulary size is\", train_corpus_amount,\"with <UNK>, <START>, <END>\")\n",
        "\n",
        "# Bigram Features: c(w,u) -- adjacent tokens pair (list\\dict)\n",
        "for curline in sentence_set:\n",
        "    for index in range(0, len(curline)+1):\n",
        "        if (index!=len(curline)) and (curline[index] not in uni_fre_dict.keys()): # should be <UNK>\n",
        "            curline[index] = '<UNK>'    \n",
        "        string_bi = None\n",
        "        if index == 0:\n",
        "            string_bi = curline[index]+\"|\"+'<START>'\n",
        "        elif index == 1:\n",
        "            string_bi = curline[index] + '|'+curline[index-1]\n",
        "        elif index == len(curline):\n",
        "            string_bi = '<END>'+'|'+curline[index-1]\n",
        "        else:\n",
        "            string_bi = curline[index] + '|' + curline[index-1]\n",
        "        V_bi[string_bi] += 1\n",
        "\n",
        "bi_fre_list, bi_unk_list = preprocess(V_bi,filter=\"bigram\",lower_bound=5)\n",
        "bi_fre_list.append(('<START>|<START>', num_sentences)) # tri-gram will use it\n",
        "bi_fre_list.append(('<START>|<START>', num_sentences))\n",
        "bi_fre_dict = dict(bi_fre_list)\n",
        "bi_unk_dict = dict(bi_unk_list)\n",
        "\n",
        "new_uni_features = build_unigram_feature(sentence_set, dimension = uni_dimension)\n",
        "new_bi_features = build_bi_feature(sentence_set, dimension = bi_dimension)\n",
        "Glove_features = build_Glove_feature(sentence_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute three metrics (P,R,F1) on dev set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model type: unigram with lrcvl1, p= 0.5822062175139213, r= 0.5791284403669725, f1= 0.5799372376266886\n",
            "model type: bigram with lrcvl1, p= 0.6149675033002652, r= 0.6146788990825688, f1= 0.6146647101364816\n",
            "model type: Glove with lrcvl1, p= 0.7982412233033858, r= 0.7981651376146789, f1= 0.7981863838369558\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, f1_score, recall_score\n",
        "\n",
        "\"\"\" \n",
        "unigram -- 150\n",
        "bigram --- 600\n",
        "model type: unigram with lrcvl1, p= 0.6252223671841266, r= 0.6238532110091743, f1= 0.6242099887320119\n",
        "model type: bigram with lrcvl1, p= 0.7910040790812248, r= 0.7878440366972477, f1= 0.7882966275768064\n",
        "model type: Glove with lrcvl1, p= 0.8112412233033858, r= 0.8091651376146789, f1= 0.8101863838369558\n",
        "\"\"\"\n",
        "\n",
        "def compute_merics(predict, realLabel,string=\"model type\"):\n",
        "    p = precision_score(predict,realLabel ,average='weighted')\n",
        "    r = recall_score(predict,realLabel,average='weighted')\n",
        "    f1 = f1_score(predict,realLabel,average='weighted')\n",
        "    print('model type: {0}, p= {1}, r= {2}, f1= {3}'.format(string,p,r,f1))\n",
        "\n",
        "uni_label_lrcvl1 = uni_lrcvl1.predict(new_uni_features)\n",
        "compute_merics(uni_label_lrcvl1, sentence_label, string=\"unigram with lrcvl1\")\n",
        "\n",
        "bi_label_lrcvl1 = bi_lrcvl1.predict(new_bi_features)\n",
        "compute_merics(bi_label_lrcvl1, sentence_label, string=\"bigram with lrcvl1\")\n",
        "\n",
        "Glove_label_lrcvl1 = Glove_lrcvl1.predict(Glove_features)\n",
        "compute_merics(Glove_label_lrcvl1, sentence_label, string=\"Glove with lrcvl1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare the performance of three types of features on dev set. Report the weighted average precision, recall and F1-score for each feature set.\n",
        "\n",
        "| Feature | precision | recall | F1-score |\n",
        "| ----------- | --------- | ------ | -------- |\n",
        "| unigram (dimension= 150)         |     0.625    |    0.624  |     0.624     |\n",
        "| bigram (dimension= 600)          |     0.791     |     0.789   |     0.788     |\n",
        "| GloVe  (load:'glove-wiki-gigaword-200')     |    0.811    |     0.809     |    0.810    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Better Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14428\\3125987708.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'glove-wiki-gigaword-100'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\gensim\\downloader.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~/gensim-data\\glove-wiki-gigaword-100\\__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'glove-wiki-gigaword-100'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'glove-wiki-gigaword-100.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1723\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1724\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1725\u001b[1;33m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1726\u001b[0m         )\n\u001b[0;32m   1727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2071\u001b[0m             )\n\u001b[0;32m   2072\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2073\u001b[1;33m             \u001b[0m_word2vec_read_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2074\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m         logger.info(\n",
            "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_read_text\u001b[1;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[0;32m   1973\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_word2vec_read_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1974\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline_no\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1975\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1976\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1977\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\gzip.py\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"B\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\gzip.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    469\u001b[0m             \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m             \u001b[0muncompress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "model = api.load('glove-wiki-gigaword-100')\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf.fit_transform(corpus)\n",
        "\n",
        "def get_sentence_embedding(sentence):\n",
        "    words = sentence.split()\n",
        "    words = [word.lower() for word in words] # convert each token to lower type\n",
        "    word_embeddings = [model.get_vector(word) for word in words if word in model]\n",
        "    \n",
        "    index = []\n",
        "    for word in words:\n",
        "        if word in model: index.append(True)\n",
        "        else: index.append(False)\n",
        "\n",
        "    # 使用TF-IDF加权嵌入\n",
        "    weights = np.array([tfidf.idf_[tfidf.vocabulary_.get(word, 0)] for word in words])\n",
        "    weights = np.expand_dims(weights, axis=1)\n",
        "    weighted_embeddings = weights[index] * np.array(word_embeddings)\n",
        "    # 取所有单词嵌入的加权平均值\n",
        "    if len(weighted_embeddings) > 0:\n",
        "        sentence_embedding = np.mean(weighted_embeddings, axis=0)\n",
        "    else:\n",
        "        sentence_embedding = np.zeros(model.vector_size)\n",
        "    return sentence_embedding\n",
        "\n",
        "# 调用函数获取句子嵌入\n",
        "sentence = \"This is a sample sentence.\"\n",
        "embedding = get_sentence_embedding(sentence)\n",
        "print(embedding)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.1 ('bert_chinese')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "53212d6bd219dd71a1dd1974b202da0e24447591239b9cfd3b7797526bde5b81"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
