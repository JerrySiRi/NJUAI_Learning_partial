{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2：\n",
    "\n",
    "⾃动编码器模型（AE）；变分⾃动编码器模型（VAE）；条件变分⾃动编码器模型（Conditional VAE）\n",
    "\n",
    "并将模型应⽤到MNIST 数据上对⼿写体数字进⾏降维、聚类、⽣成和指定⽣成⼯作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FC layer实现AE、VAE、CVAE & 画图展示\n",
    "\n",
    "### 1.1 AE、VAE、CVAE架构搭建\n",
    " \n",
    "#### 1.1.1 通用部件Encoder & Decoder的实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Encoder实现\n",
    "\n",
    "condition_size = 10 #10个数字，用一个10维向量作为CVAE的条件（0-9十个数字，依label给出one-hot编码）\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def onehot(label): # CVAE为了要和input（1*28*28）可以concatenate，需要扩展成3维的(后续还有batch_size就变成三维)\n",
    "    label = label.unsqueeze(1)\n",
    "    label = label.unsqueeze(2)\n",
    "    #print(label.size(0))\n",
    "    exp_vec = torch.zeros(label.size(0), 1, condition_size).to(device)\n",
    "    # 【新创建的，和已在cuda上tensor无关，不知道应该放在哪里！！！】\n",
    "    exp_vec.scatter_(2, label, 1) # 根据label，对exp_vec进行操作，填写到维度为1的对应位置\n",
    "    return exp_vec\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, x_dim, hidden_size, latent_size, arch=\"AE\", **kwargs) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "        self.mu = nn.Sequential(nn.Linear(x_dim, hidden_size), nn.ReLU(), nn.Linear(hidden_size, latent_size),)\n",
    "        self.arch = arch\n",
    "        if self.arch != \"AE\":  # 若encoder返回的是均值与标准差(VAE、CVAE), 需要额外生成mu和sigma（两架构可以一样）\n",
    "            self.sigma = nn.Sequential(nn.Linear(x_dim, hidden_size), nn.ReLU(), nn.Linear(hidden_size, latent_size),)\n",
    "\n",
    "        if self.arch == \"CVAE\":\n",
    "            self.mu = nn.Sequential(nn.Linear(x_dim+condition_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, latent_size),)\n",
    "            self.sigma = nn.Sequential(nn.Linear(x_dim+condition_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, latent_size),)\n",
    "\n",
    "    def forward(self, xs, label=None):# 只有CVAE才会传入label，xs是输入的图片，784*1*batchsize\n",
    "        # 实现编码器的forward过程，arch架构的不同取值意味着我们需要不同输出的encoder\n",
    "\n",
    "        if self.arch == \"AE\":\n",
    "            output = self.mu(xs) # AE的输出，只有一个code\n",
    "        elif self.arch == \"VAE\":# VAE的输出，有mu和sigma两个输出\n",
    "            mu = self.mu(xs)\n",
    "            sigma = self.sigma(xs)\n",
    "            output = (mu,sigma)\n",
    "        else: # CVAE的输出，有mu和sigma两个，但是输入还有希望生成数据的label！先扩展、再concatenate起来\n",
    "            label = onehot(label)\n",
    "            # print(\"dim_image\",xs.shape)\n",
    "            # print(\"label_image\",label.shape)\n",
    "            xs= torch.cat((xs, label), dim=-1)\n",
    "            # xs = xs.to(device)     # Bug：不用等号赋值，就没办法移动到cuda上！\n",
    "            mu = self.mu(xs)\n",
    "            sigma = self.sigma(xs)\n",
    "            output = (mu,sigma)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decoder的实现\n",
    "# 不同的自编码器可能需要不同的输入, 借助**kwargs来处理.\n",
    "# Review：**收集所有未匹配的关键字参数组成一个dict对象，局部变量kwargs指向此dict对象\n",
    "# 对于AE的情况, 只需要zs作为输入即可,\n",
    "# 对于VAE,CVAE的情况, 我们可能需要code服从分布的均值与方差(对于CVAE, 还需要类别的指示变量)\n",
    "\n",
    "# 在实现遇到迷茫的时候, 不妨考虑具体的自编码器类需要什么样的encoder, decoder.\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, x_dim, hidden_size, latent_size, arch=\"AE\", **kwargs) -> None:\n",
    "        super(Decoder, self).__init__()\n",
    "        self.arch = arch\n",
    "        if self.arch == \"AE\":\n",
    "            self.decoder = nn.Sequential(nn.Linear(latent_size, hidden_size), nn.ReLU(), \n",
    "            nn.Linear(hidden_size, x_dim),)\n",
    "        elif self.arch == \"VAE\": # 需要传入encoder的均值和方差 \n",
    "            self.decoder = nn.Sequential(nn.Linear(latent_size, hidden_size), nn.ReLU(), \n",
    "            nn.Linear(hidden_size, x_dim),)\n",
    "        elif self.arch == \"CVAE\":\n",
    "            self.decoder = nn.Sequential(nn.Linear(latent_size+condition_size, hidden_size), nn.ReLU(), \n",
    "            nn.Linear(hidden_size, x_dim),)  \n",
    "\n",
    "    def forward(self, zs,label=None, **otherinputs): # 【【此时的zs已经是经过处理过的，已经是N(mu,sigma^2)中sample出来的值了】】\n",
    "        # 实现decoder的decode部分,此时就假定直接输出图片了，不再生成mu和sigma再sample\n",
    "        if self.arch == \"AE\":\n",
    "            output = self.decoder(zs)\n",
    "        elif self.arch == \"VAE\":\n",
    "            output = self.decoder(zs)\n",
    "        elif self.arch == \"CVAE\":\n",
    "            label = onehot(label)\n",
    "            zs = torch.cat((zs, label), dim=-1)\n",
    "            output = self.decoder(zs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 基于通用部件的AE、VAE、CVAE实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, **kwargs) -> None:\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "class VAE(nn.Module): # 要同时返回中间参数mu和sigma的，用于后续计算kl loss\n",
    "    def __init__(self,encoder,decoder,**kwargs) -> None:\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self,x):\n",
    "        mu,sigma = self.encoder(x) # 【encoder的forward函数返回了两个参数！！！】\n",
    "        # epsilon = torch.randn(mu) # 生成了很多个正态分布！！！mu是一个latent_size大小的，每一个分量是一个正态参数\n",
    "        # z = mu + epsilon*sigma\n",
    "        # 【更优！加入随机噪声！！！对encoder输出进行简单处理，且保证是正数！】\n",
    "        std = torch.exp(0.5 * sigma)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        z = epsilon * std + mu\n",
    "        return mu, sigma, self.decoder(z)\n",
    "\n",
    "    def generate(self,new_code):\n",
    "        new_code = new_code.to(device) \n",
    "        return self.decoder(new_code) # 【假设此时new_code被用户经过正态分布的处理了！】\n",
    "\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self,encoder,decoder,**kwargs) -> None:\n",
    "        super(CVAE, self).__init__()\n",
    "        self.encoder = encoder #【【【小心！传入时arch必须设定成 “CVAE”】】】\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self,x,label):# label必须传入\n",
    "        mu,sigma =self.encoder(x,label)# 【forward返回两参数，其中input已经concatenate了】\n",
    "        std = torch.exp(0.5 * sigma)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        z = epsilon * std + mu\n",
    "        return mu, sigma, self.decoder(z,label) # decoder中再合并\n",
    "\n",
    "    def generate(self,new_code,label): # 【假设此时new_code被用户经过正态分布的处理了！label就是一个数字】\n",
    "        new_code = new_code.to(device)\n",
    "        label = label.to(device)\n",
    "        return self.decoder(new_code,label)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 模型训练\n",
    "\n",
    "## 1.2.1 损失函数\n",
    "其中值得注意的是VAE及CVAE为了防止过拟合而引入的KL散度，作为损失函数的衡量标准之一\n",
    "$$KL(\\mathcal{N}(\\mu,\\sigma ^2)||\\mathcal{N}(0,1) ) = -\\frac{1}{2}(\\log (\\sigma ^2)-\\sigma ^2-\\mu^2+1) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练中损失函数\n",
    "# AE：MSE\n",
    "# VAE & CVAE：KL loss + MSE \n",
    "\n",
    "# 【可调节超参数：kl散度在epoch增加后所占loss的权重】\n",
    "aging_rate = 0.3\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def __init__(self, arch=\"AE\", choice=\"BCE\", **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.arch = arch\n",
    "        self.choice = choice\n",
    "    def recon_loss(self, x, x_): # 重构损失\n",
    "        if self.choice == \"L1\": # L1\n",
    "            instance = nn.L1Loss()  # 【【BUG？必须选择这个？】】\n",
    "            loss = instance(x, x_) # 【【类的实例化（上一行） 和 实例的调用（本行）不能放在一起！！！】】\n",
    "        else: # BCE\n",
    "            loss = F.binary_cross_entropy(F.sigmoid(x_),F.sigmoid(x), reduction='sum')\n",
    "        return loss\n",
    "\n",
    "    def kl_div(self, mu,sigma): # KL loss\n",
    "        return  -0.5 * torch.mean(torch.log(sigma.pow(2)) - sigma.pow(2) - mu.pow(2) + 1)\n",
    "\n",
    "    def forward(self, x, x_, mu=None, sigma=None, **otherinputs):# loss的计算\n",
    "        loss = Loss.recon_loss(self,x,x_)# 可选choice = \"L1\"\n",
    "        if self.arch == \"AE\":\n",
    "            total_loss = loss\n",
    "        elif self.arch == \"VAE\" or self.arch==\"CVAE\":\n",
    "            kl_loss = Loss.kl_div(self,mu,sigma)\n",
    "            total_loss = loss + kl_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数，可以在x.reshape中调整！\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "def train_epoch(model, train_loader, loss, optimizer, epoch, epoch_num, arch):\n",
    "    \"\"\"\n",
    "    main中需要传入的参数\n",
    "    model：是AE、VAE、CVAE的model（类的实例）\n",
    "    train_loader：main中导入的minist数据集\n",
    "    loss：定义的损失函数类的实例中的函数\n",
    "    optimizer：torch中自带的优化器\n",
    "    epoch：当前迭代优化轮数\n",
    "    epoch_num：一共迭代优化的轮数\n",
    "    arch：当前选择的model的名字（字符串）\n",
    "    \"\"\"\n",
    "    model.train() # 调用继承的父类nn.Module的方法，所有子模块调整为训练模式\n",
    "    train_loss = 0\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        # 【【【只需要把所有训练数据（x，y）放到cuda上就可以啦！！！其他的自动依据device判断位置】】】\n",
    "        y = y.to(device)\n",
    "        # Review：enumerate把可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标\n",
    "        # 既可以直接访问该对象，又可以通过索引来访问！\n",
    "        x = x.reshape(128, 1, -1)  \n",
    "        # 生成(128,1,784)，原来batch_size对应的特征矩阵是128*1*28*28，\n",
    "        # 其实四维和三维的效果一样，28*28每个pixel都是要作为一个neuron作为input的！\n",
    "        if arch == \"AE\":\n",
    "            # x_ = model.forward(x)\n",
    "            x_ = model(x)     \n",
    "            \"\"\"\n",
    "            自动调用 forward 函数原因分析：\n",
    "            利用Python的语言特性，y = model(x)是调用了对象model的__call__方法，\n",
    "            而nn.Module把__call__方法实现为类对象的forward函数，\n",
    "            所以任意继承了nn.Module的类对象都可以这样简写来调用forward函数。\n",
    "            \"\"\"\n",
    "            loss_num = loss(x, x_) # 同理，自动调用loss中的forward方法！！！\n",
    "        elif  arch == \"VAE\":\n",
    "            # mu, log_var, x_= model.forward(x)\n",
    "            mu, sigma, x_= model(x)\n",
    "            loss_num = loss(x, x_,mu,sigma)\n",
    "        elif  arch == \"CVAE\":\n",
    "            # mu, log_var, x_ = model.forward(x,y)# y是train data的真实label\n",
    "            mu, sigma, x_ = model(x,y)# y是train data的真实label\n",
    "            loss_num = loss(x, x_,mu,sigma)\n",
    "        train_loss += loss_num # 在此处计算总loss\n",
    "        optimizer.zero_grad() # 在计算本batch_size的梯度之前，把之前的梯度清空,不做积累。只针对每一个batch中loss回溯\n",
    "        loss_num.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 0: # 训练过程中的标记\n",
    "            print('Train Epoch:',epoch, i * len(x), len(train_loader.dataset),\n",
    "                       100. * i / len(train_loader),\"Loss: \",loss_num.item())\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch,\n",
    "        train_loss * 128 / len(train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 性能评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.utils import make_grid, save_image\n",
    "# make_grid是张量可视化的包！！！\n",
    "# 可以协同plt一起共同操作，可以操作一下！！！\n",
    "\n",
    "def evaluate_epoch(model, test_loader, loss, epoch, epoch_num, arch):\n",
    "    model.eval() # 所有子模块调整为评估模式\n",
    "    test_loss = 0\n",
    "    with torch.no_grad(): # 评估过程，不用调整参数=不用梯度下降》》在此作用域中不会构建计算图\n",
    "        for i, (x, y) in enumerate(test_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)#【必须要放！】\n",
    "            x = x.reshape(128, 1, -1) # 同理，特征矩阵的三维处理\n",
    "\n",
    "            if arch == \"AE\":\n",
    "                x_ = model(x)\n",
    "                loss_num = loss(x, x_)\n",
    "                test_loss += loss_num.item()\n",
    "\n",
    "                # 保存一些重构出来的图像\n",
    "                with torch.no_grad():\n",
    "                    if i % 20 == 0:\n",
    "                        # 对照：保留原始图像数据，每50个保留一个\n",
    "\n",
    "                        # 测试：对原图的重现功能\n",
    "                        if not os.path.exists(f\"results_AE/epoch_{epoch}\"):\n",
    "                            os.makedirs(f\"results_AE/epoch_{epoch}\")\n",
    "                        save_x_ = x_.reshape(-1, 1, 28, 28)[:16]\n",
    "                        save_x_ = make_grid(save_x_, 8, 0)\n",
    "                        save_image(save_x_, os.path.join(f\"results_AE/epoch_{epoch}/batch_{i}.png\"))\n",
    "\n",
    "                        if not os.path.exists(f\"results_AE/initial_data\"):\n",
    "                            os.makedirs(f\"results_AE/initial_data\")\n",
    "                        save_x = x.reshape(-1,1, 28, 28)[:16]#dim=0是batch_size，后面就是一个个图片的三维大小啦\n",
    "                        save_x = make_grid(save_x,8,0) # 【生成图片，可以后续使用plt在notebook展现】\n",
    "                        save_image(save_x,os.path.join(f\"results_AE/initial_data/batch_{i}.png\"))\n",
    "\n",
    "            # 只有VAE和CVAE有图像生成功能，AE就不用考虑啦！\n",
    "            elif arch == \"VAE\":\n",
    "                mu, sigma, x_ = model(x)\n",
    "                loss_num = loss(x, x_, mu, sigma)\n",
    "                test_loss += loss_num.item()\n",
    "                z = torch.randn(128,1,10)\n",
    "\n",
    "                # 测试：随机向量生成图片功能\n",
    "                sample = model.generate(z)\n",
    "                sample = sample.reshape(-1, 1, 28, 28)[:32]\n",
    "                #print(z[0],sample[0])\n",
    "                if not os.path.exists(f\"results_VAE\"):\n",
    "                    os.makedirs(f\"results_VAE\")\n",
    "                save_image(\n",
    "                    sample,\n",
    "                    os.path.join(f\"results_VAE/epoch_{epoch}.png\")\n",
    "                )\n",
    "                # 【生成图片，可以后续使用plt在notebook展现】\n",
    "\n",
    "            elif arch == \"CVAE\":\n",
    "                mu, sigma, x_ = model(x,y)\n",
    "                loss_num = loss(x, x_, mu, sigma)\n",
    "                test_loss += loss_num.item()\n",
    "                z = torch.randn(128,1,10)\n",
    "\n",
    "                # 测试：随机向量生成图片功能\n",
    "                sample = model.generate(z,y)\n",
    "                sample = sample.reshape(-1, 1, 28, 28)[:32]\n",
    "                #print(z[0],sample[0])\n",
    "                # 保存一些重构出来的图像用于(写报告)进一步研究 (5/100)\n",
    "                if not os.path.exists(f\"results_CVAE\"):\n",
    "                    os.makedirs(f\"results_CVAE\")\n",
    "                save_image(\n",
    "                    sample,\n",
    "                    os.path.join(f\"results_CVAE/epoch_{epoch}.png\")\n",
    "                )\n",
    "                # 【生成图片，可以后续使用plt在notebook展现】\n",
    "\n",
    "    test_loss *= 128\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集格式：28*28的灰度图像，channel = 1\n",
    "# 超参数 batch_size = 128\n",
    "\n",
    "import torch.utils.data as td\n",
    "import torchvision.datasets.mnist as mnist\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.linalg as linalg\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_data(wh_train=True, batch_size=128):\n",
    "    dataset = mnist.MNIST(\n",
    "        root=\"./data\",\n",
    "        train=wh_train,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                #transforms.ToPILImage(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.1307,], [0.3081,],),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    #for i, (x, y) in enumerate(dataset):\n",
    "        #print(i,x.shape,y)# 0 torch.Size([1, 28, 28]) \n",
    "        #print(y.shape)\n",
    "    return td.DataLoader(dataset, batch_size=batch_size,drop_last=True,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 main函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/19/2023 13:35:52 - INFO - __main__ -   device: cuda n_gpu: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=128, device=device(type='cuda'), epoch_num=128, hidden_size=128, latent_size=10, type='VAE', x_dim=784)\n",
      "\n",
      " epoch: 0\n",
      "Train Epoch: 0 0 60000 0.0 Loss:  2.25697922706604\n",
      "Train Epoch: 0 6400 60000 10.683760683760683 Loss:  0.6273771524429321\n",
      "Train Epoch: 0 12800 60000 21.367521367521366 Loss:  0.535597026348114\n",
      "Train Epoch: 0 19200 60000 32.05128205128205 Loss:  0.689893901348114\n",
      "Train Epoch: 0 25600 60000 42.73504273504273 Loss:  0.5740326046943665\n",
      "Train Epoch: 0 32000 60000 53.41880341880342 Loss:  0.5111684799194336\n",
      "Train Epoch: 0 38400 60000 64.1025641025641 Loss:  0.45639461278915405\n",
      "Train Epoch: 0 44800 60000 74.78632478632478 Loss:  0.462720662355423\n",
      "Train Epoch: 0 51200 60000 85.47008547008546 Loss:  0.5798219442367554\n",
      "Train Epoch: 0 57600 60000 96.15384615384616 Loss:  0.47290411591529846\n",
      "====> Epoch: 0 Average loss: 0.5627\n",
      "====> Test set loss: 0.5021\n",
      "\n",
      " epoch: 1\n",
      "Train Epoch: 1 0 60000 0.0 Loss:  0.4960477352142334\n",
      "Train Epoch: 1 6400 60000 10.683760683760683 Loss:  0.5128631591796875\n",
      "Train Epoch: 1 12800 60000 21.367521367521366 Loss:  0.45207637548446655\n",
      "Train Epoch: 1 19200 60000 32.05128205128205 Loss:  0.5088237524032593\n",
      "Train Epoch: 1 25600 60000 42.73504273504273 Loss:  1.1800317764282227\n",
      "Train Epoch: 1 32000 60000 53.41880341880342 Loss:  0.5461779236793518\n",
      "Train Epoch: 1 38400 60000 64.1025641025641 Loss:  0.6931021809577942\n",
      "Train Epoch: 1 44800 60000 74.78632478632478 Loss:  0.500354528427124\n",
      "Train Epoch: 1 51200 60000 85.47008547008546 Loss:  0.5075854659080505\n",
      "Train Epoch: 1 57600 60000 96.15384615384616 Loss:  0.5808101892471313\n",
      "====> Epoch: 1 Average loss: 0.5901\n",
      "====> Test set loss: 0.6144\n",
      "\n",
      " epoch: 2\n",
      "Train Epoch: 2 0 60000 0.0 Loss:  0.6337330341339111\n",
      "Train Epoch: 2 6400 60000 10.683760683760683 Loss:  0.5166402459144592\n",
      "Train Epoch: 2 12800 60000 21.367521367521366 Loss:  0.46479421854019165\n",
      "Train Epoch: 2 19200 60000 32.05128205128205 Loss:  0.4991552233695984\n",
      "Train Epoch: 2 25600 60000 42.73504273504273 Loss:  0.4852498173713684\n",
      "Train Epoch: 2 32000 60000 53.41880341880342 Loss:  1.087031602859497\n",
      "Train Epoch: 2 38400 60000 64.1025641025641 Loss:  0.45329591631889343\n",
      "Train Epoch: 2 44800 60000 74.78632478632478 Loss:  0.4535006880760193\n",
      "Train Epoch: 2 51200 60000 85.47008547008546 Loss:  0.5008532404899597\n",
      "Train Epoch: 2 57600 60000 96.15384615384616 Loss:  0.47065022587776184\n",
      "====> Epoch: 2 Average loss: 0.5607\n",
      "====> Test set loss: 0.4738\n",
      "\n",
      " epoch: 3\n",
      "Train Epoch: 3 0 60000 0.0 Loss:  0.46887314319610596\n",
      "Train Epoch: 3 6400 60000 10.683760683760683 Loss:  0.48380574584007263\n",
      "Train Epoch: 3 12800 60000 21.367521367521366 Loss:  0.5594514012336731\n",
      "Train Epoch: 3 19200 60000 32.05128205128205 Loss:  0.5184601545333862\n",
      "Train Epoch: 3 25600 60000 42.73504273504273 Loss:  0.4780404269695282\n",
      "Train Epoch: 3 32000 60000 53.41880341880342 Loss:  0.48312854766845703\n",
      "Train Epoch: 3 38400 60000 64.1025641025641 Loss:  0.4312732517719269\n",
      "Train Epoch: 3 44800 60000 74.78632478632478 Loss:  0.5050116181373596\n",
      "Train Epoch: 3 51200 60000 85.47008547008546 Loss:  0.521443247795105\n",
      "Train Epoch: 3 57600 60000 96.15384615384616 Loss:  0.49065306782722473\n",
      "====> Epoch: 3 Average loss: 0.5081\n",
      "====> Test set loss: 0.5045\n",
      "\n",
      " epoch: 4\n",
      "Train Epoch: 4 0 60000 0.0 Loss:  0.47973987460136414\n",
      "Train Epoch: 4 6400 60000 10.683760683760683 Loss:  0.48171424865722656\n",
      "Train Epoch: 4 12800 60000 21.367521367521366 Loss:  0.4646906554698944\n",
      "Train Epoch: 4 19200 60000 32.05128205128205 Loss:  0.49174758791923523\n",
      "Train Epoch: 4 25600 60000 42.73504273504273 Loss:  0.48564839363098145\n",
      "Train Epoch: 4 32000 60000 53.41880341880342 Loss:  0.5401738882064819\n",
      "Train Epoch: 4 38400 60000 64.1025641025641 Loss:  0.4163254201412201\n",
      "Train Epoch: 4 44800 60000 74.78632478632478 Loss:  0.4395654499530792\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5600\\33700094.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;31m# parser.set_defaults(type=\"CVAE\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;31m# args = parser.parse_args()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5600\\33700094.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n epoch:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         train_epoch(model=auto_encoder, loss=loss, train_loader=train_loader,\n\u001b[1;32m---> 31\u001b[1;33m                     optimizer=optimizer, epoch=epoch, epoch_num=args.epoch_num, arch=args.type)\n\u001b[0m\u001b[0;32m     32\u001b[0m         evaluate_epoch(model=auto_encoder, test_loader=test_loader, loss=loss,\n\u001b[0;32m     33\u001b[0m                    epoch=epoch, epoch_num=args.epoch_num, arch=args.type)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5600\\3533259602.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_loader, loss, optimizer, epoch, epoch_num, arch)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 调用继承的父类nn.Module的方法，所有子模块调整为训练模式\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# 【【【只需要把所有训练数据（x，y）放到cuda上就可以啦！！！其他的自动依据device判断位置】】】\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \"\"\"\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'std evaluated to zero after conversion to {}, leading to division by zero.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86152\\anaconda3\\envs\\bert_chinese\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    encoder_args = {\n",
    "        \"x_dim\": args.x_dim,\n",
    "        \"hidden_size\": args.hidden_size,\n",
    "        \"latent_size\": args.latent_size,\n",
    "        \"arch\": args.type,\n",
    "    }\n",
    "    encoder = Encoder(**encoder_args)\n",
    "    decoder = Decoder(**encoder_args)\n",
    "    ae = {\"AE\": AE, \"VAE\": VAE, \"CVAE\": CVAE}\n",
    "\n",
    "    auto_encoder = ae[args.type](encoder, decoder) #【巧！通过字典的索引直接可以用此类来创建实例啦！】\n",
    "\n",
    "    auto_encoder.to(device)\n",
    "    # 优化器选择\n",
    "    # optimizer = torch.optim.SGD(auto_encoder.parameters(), lr=0.01)\n",
    "    optimizer =torch.optim.Adam(auto_encoder.parameters(), lr=0.001)\n",
    "    # class中用nn定义的神经网络中参数，可认为是自动加上了nn.Parameters()的处理，参数会被加入！\n",
    "    train_loader = get_data(wh_train=True, batch_size=args.batch_size)\n",
    "    test_loader = get_data(wh_train=False, batch_size=args.batch_size)\n",
    "    loss = Loss(args.type,choice=\"L1\")#【不加choice，就默认用BCE.可以选择choice=\"L1\"】\n",
    "\n",
    "# TODO：可以每个损失函数，分别给一个文件夹存放出来的图片呢！！！\n",
    "\n",
    "    for epoch in range(args.epoch_num):\n",
    "        print(\"\\n epoch:\", epoch)\n",
    "        train_epoch(model=auto_encoder, loss=loss, train_loader=train_loader,\n",
    "                    optimizer=optimizer, epoch=epoch, epoch_num=args.epoch_num, arch=args.type)\n",
    "        evaluate_epoch(model=auto_encoder, test_loader=test_loader, loss=loss,\n",
    "                   epoch=epoch, epoch_num=args.epoch_num, arch=args.type)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    matplotlib.use('TkAgg')\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--type\", default=\"VAE\", choices=[\"AE\", \"VAE\", \"CVAE\"])\n",
    "    parser.add_argument(\"--x_dim\", default=784, type=int)  # 28 x 28 的像素展开为一个一维的行向量，每行代表一个图片\n",
    "    parser.add_argument(\"--latent_size\", default=10, type=int)  # 输出层大小，即服从高斯分布的隐含变量的维度。\n",
    "    parser.add_argument(\"--hidden_size\", default=128, type=int)\n",
    "    parser.add_argument(\"--batch_size\", default=128, type=int)\n",
    "    parser.add_argument(\"--epoch_num\", default=128, type=int)\n",
    "    # args = parser.parse_args()\n",
    "    # 调用parser.parse_args()会读取系统参数：sys.argv[]，命令行调用时是正确参数，\n",
    "    # 而在jupyter notebook中调用时，sys.argv的值为ipykrnel_launcher.py：\n",
    "\n",
    "    # args = parser.parse_args(args=[])\n",
    "    \n",
    "    args = parser.parse_known_args()[0]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    args.device = device\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                        datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                        level = logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"device: {device} n_gpu: {n_gpu}\")\n",
    "\n",
    "    print(args)\n",
    "    main(args)\n",
    "    # parser.set_defaults(type=\"CVAE\")\n",
    "    # args = parser.parse_args()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('bert_chinese')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53212d6bd219dd71a1dd1974b202da0e24447591239b9cfd3b7797526bde5b81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
